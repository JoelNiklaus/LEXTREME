{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json as js\n",
    "from collections import Counter\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- To use fasttext, go to https://fasttext.cc/docs/en/language-identification.html and download the model `lid.176.bin` and move it to the folder `models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all necessary functions and classes\n",
    "\n",
    "import fasttext\n",
    "\n",
    "class LanguageIdentification:\n",
    "    \n",
    "    '''From: https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = \"../models/lid.176.bin\"\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def predict_lang(self, text):\n",
    "        predictions = self.model.predict(text, k=2) # returns top 2 matching languages\n",
    "        return re.sub('__label__','',predictions[0][0])\n",
    "    \n",
    "language_detector = LanguageIdentification()\n",
    "\n",
    "def remove_if_not_required_language(text, required_language):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub('\\n','', text)\n",
    "    \n",
    "    predicted_language = language_detector.predict_lang(text)\n",
    "    \n",
    "    if predicted_language==required_language:\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "\n",
    "\n",
    "def convert_topics_to_list(affair_topic_codes):\n",
    "    \n",
    "    '''Converts the string of semicolon-seperated topic ids into a list'''\n",
    "    \n",
    "    topic_list = str(affair_topic_codes).split(';')\n",
    "    topic_list = [x for x in topic_list if bool(re.search('\\w',x))]\n",
    "    return topic_list \n",
    "\n",
    "\n",
    "def strip_html_tags(input_text):\n",
    "    \n",
    "    if type(input_text)==str:\n",
    "        soup = BeautifulSoup(input_text)\n",
    "        return soup.text\n",
    "    else:\n",
    "        return input_text\n",
    "    \n",
    "    \n",
    "# Creating json with id to label and label to id\n",
    "\n",
    "label_df = pd.read_excel('../data/topics_politmonitor.xlsx')\n",
    "label_df['keyword_id']=label_df.keyword_id.apply(lambda x: int(x))\n",
    "label2id = dict()\n",
    "id2label = dict()\n",
    "for record in label_df.to_dict(orient=\"record\"):\n",
    "    _id = int(record['keyword_id'])\n",
    "    label = record['keyword_de']\n",
    "    label2id[label]=_id\n",
    "    id2label[_id]=label\n",
    "\n",
    "with open('../utils/id2label.json', 'w') as f:\n",
    "    js.dump(id2label,f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "with open('../utils/label2id.json', 'w') as f:\n",
    "    js.dump(label2id,f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Merging the different data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affairs = pd.read_excel('../data/Data/affairs.xlsx')\n",
    "\n",
    "# Filtering out the following cases\n",
    "    # topics that are nan\n",
    "    # cases where topic id is 1696==Diverses and topic is other\n",
    "    \n",
    "affairs = affairs[affairs.affair_topic_codes.isnull()==False]\n",
    "affairs = affairs[affairs.affair_topic_codes.isin(['1696','other'])==False]\n",
    "affairs[\"affair_topic_codes\"] = affairs.affair_topic_codes.progress_apply(convert_topics_to_list)\n",
    "\n",
    "#affairs = affairs[(affairs.affair_scope.isin(['zh','ch']))] # We will keep all kantons for now\n",
    "\n",
    "affairs = affairs[(affairs.affair_topic_codes.apply(lambda x : len(x)>0))]\n",
    "affairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(affairs.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read other text parts from csv\n",
    "\n",
    "\n",
    "\n",
    "all_textual_data = list()\n",
    "for kanton in affairs.affair_scope.unique():\n",
    "    \n",
    "    print('+++ Processing kanton '+kanton +' +++')\n",
    "    \n",
    "    file_name = '../data/Data/documents_content/'+kanton+'.csv.gz'\n",
    "    if os.path.isfile(file_name):\n",
    "        df = pd.read_csv(file_name, compression='gzip', sep=\";\")\n",
    "        df['affair_text_scope'] = kanton\n",
    "        for x in [\"text_de\", \"text_fr\", \"text_it\"]:\n",
    "            df[x] = df[x].apply(strip_html_tags)\n",
    "        all_textual_data.append(df)\n",
    "        text_names_de.add()\n",
    "    else:\n",
    "        print('The following file does not exists: ', file_name)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "all_textual_data_df = pd.concat(all_textual_data)\n",
    "\n",
    "#Reset the index\n",
    "all_textual_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See unique values of text_name\n",
    "\n",
    "text_names_de = set()\n",
    "text_names_fr = set()\n",
    "text_names_it = set()\n",
    "\n",
    "for x in all_textual_data_df.to_dict(orient=\"records\"):\n",
    "    text_names_de.add(x['text_name_de'])\n",
    "    text_names_fr.add(x['text_name_fr'])\n",
    "    text_names_it.add(x['text_name_it'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all content where, for example, the field is text_de, but the actual text is not German\n",
    "\n",
    "for language in ['de', 'fr', 'it']:\n",
    "    all_textual_data_df['text_'+language]=all_textual_data_df['text_'+language].progress_apply(lambda x: remove_if_not_required_language(x,language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a column to merge the df created from the csv with the df for the affairs\n",
    "# The column will be created by concatinating \n",
    "#     affair_scope with affair_scope with affair_srcid (for affairs)\n",
    "#     affair_scope with affair_text_scope with affair_text_srcid (for csv data)\n",
    "\n",
    "column_name_for_merge = 'id_for_merge'\n",
    "#affairs[column_name_for_merge] = affairs['affair_scope']+'_'+affairs['affair_srcid'].apply(lambda x: str(x))\n",
    "#all_textual_data_df[column_name_for_merge] = all_textual_data_df['affair_text_scope']+'_'+all_textual_data_df['affair_text_srcid'].apply(lambda x: str(x))\n",
    "\n",
    "affairs[column_name_for_merge] = affairs['affair_srcid']\n",
    "all_textual_data_df[column_name_for_merge] = all_textual_data_df['affair_text_srcid']\n",
    "\n",
    "# Then, in the dataframe for the csv data, we keep only those columns that do not occur in the affairs dataframe\n",
    "\n",
    "columns_not_in_affairs = [c for c in all_textual_data_df.columns.tolist() if c not in affairs.columns.tolist()] + [column_name_for_merge]\n",
    "\n",
    "all_textual_data_df_for_merge = all_textual_data_df[columns_not_in_affairs]\n",
    "\n",
    "df_merged = all_textual_data_df_for_merge.merge(affairs,on=column_name_for_merge)\n",
    "\n",
    "df_merged['affair_topic_codes_as_labels'] = df_merged.affair_topic_codes.apply(lambda label_list: [id2label[int(x)] for x in label_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "df_merged.to_json('../data/raw_data_for_training.jsonl', lines=True, orient=\"records\", force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating overview of the frequency of all labels\n",
    "\n",
    "all_labels = list()\n",
    "for label_list in df_merged.affair_topic_codes_as_labels:\n",
    "    for label in label_list:\n",
    "        all_labels.append(label)\n",
    "        \n",
    "plot_data = pd.DataFrame([dict(Counter(all_labels))]) \n",
    "plot_data = plot_data.sort_values(by=0, ascending=False, axis=1)\n",
    "plot_data.plot.bar(figsize = (20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
