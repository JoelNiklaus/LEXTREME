{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_data_handler import TrainingDataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdh = TrainingDataHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Raw training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fields\n",
    "\n",
    "- One of the tasks is to look which text input fields suffice for good predictions\n",
    "- The important text fields can be in German, Italien or French  and are \n",
    "  - `title`: Title of each document\n",
    "  - `text`: Text of the document. This field will get a further division on the basis of the field `text_name_de` which describes whether it is an attachment, an answer etc.\n",
    "- In addition to that, we have the field `language` which can be de, fr, it. This field is important to filter the entries by language\n",
    "- The dataframe contains the original fields, which are not used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdh.raw_data_for_training.head().columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'all'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], affair_attachment_category='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdh.training_data_df[tdh.training_data_df.text.str.contains('Infolge der geÃ¤nderten Gesetzgebung betreffend die Quellenbesteuerung des Einkommens aus ErwerbstÃ¤tigkeit wird in Zukunft auch GrenzgÃ¤ngerinnen und GrenzgÃ¤ngern als \"Quasi-An')].affair_topic_codes_as_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(list(tdh.label2id.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check if everything is all right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affairs = pd.read_excel('/Users/vetonmatoshi/Documents/Git/LEXTREME/politmonitor/data/Data/affairs.xlsx')\n",
    "affairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will check if the topic labels per affair srcid are equal in all dataframes\n",
    "- We will take the affair_srcid from `affairs` and filter the training data by this affair_srcid\n",
    "- The field `affair_topic_codes` in the training data and `affairs` should be equal in both cases\n",
    "- The field `affair_topic_codes_as_labels` in the training data should be equal to `affair_topic_de` in affairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miss_match = list()\n",
    "for item in affairs.to_dict(orient='records'):\n",
    "    affair_srcid = item['affair_srcid'] \n",
    "    affair_topic_codes = set(str(item['affair_topic_codes']).split(';'))\n",
    "    affair_topic_de = set(str(item['affair_topic_de']).split(';'))\n",
    "    \n",
    "    # Filter training data\n",
    "    res = tdh.training_data_df[tdh.training_data_df.affair_text_srcid==affair_srcid]\n",
    "    if res.shape[0]>0:\n",
    "        affair_topic_codes_found = res.affair_topic_codes.tolist()[0]\n",
    "        affair_topic_codes_found = set(affair_topic_codes_found)\n",
    "        if not affair_topic_codes_found==affair_topic_codes:\n",
    "            print(affair_srcid)\n",
    "            miss_match.append(affair_topic_codes_found)\n",
    "        affair_topic_codes_as_labels_found = res.affair_topic_codes_as_labels.tolist()[0]\n",
    "        affair_topic_codes_as_labels_found = set(affair_topic_codes_as_labels_found)\n",
    "        if not affair_topic_codes_as_labels_found==affair_topic_de:\n",
    "            print(affair_srcid)\n",
    "            print('Actual value:', affair_topic_de, affair_topic_codes)\n",
    "            print('Found value:', affair_topic_codes_as_labels_found, affair_topic_codes_found)\n",
    "            print('#############################')\n",
    "            miss_match.append(affair_topic_codes_as_labels_found)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like if affair_text_srcid==affair_srcid, then the affair_topic_codes are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "df_count = defaultdict(int)\n",
    "for c in [x for x in columns if x!='title']:\n",
    "    df_count[c]=df[c].sum()\n",
    "\n",
    "    \n",
    "df_new_count = defaultdict(int)\n",
    "for c in [x for x in columns if x!='title']:\n",
    "    df_new_count[c]=df_new[c].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('It has cases with an empty text field: ', tdh.raw_data_for_training[tdh.raw_data_for_training.text==''].shape[0], 'cases')\n",
    "print('\\n')\n",
    "print('But it has no cases with an empty title field: ', tdh.raw_data_for_training[tdh.raw_data_for_training.title==''].shape[0], 'cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The filtering is done on the basis of the following fields from `tdh.raw_data_for_training`: `language`, `title`, `text`, `affair_text_scope`\n",
    "- All of these fields have default values. The default values will choose every available data\n",
    "- Having default values means that you do not have to specify every field for filtering\n",
    "- The filtering is done with the following methods:\n",
    "  - `tdh.get_training_data(languages, affair_text_scope, inputs)`: will return a dataset that can be used for training with the transformers library. The dataset can be accessed via tdh.traing_data\n",
    "  - `tdh.filter_training_data(languages, affair_text_scope, inputs)`: will return a pandas dataframe\n",
    "- The following arguments are important:\n",
    "  - `languages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "language = 'de'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], text=False)\n",
    "print('Number of examples for '+language+': ', tdh.training_data_df.shape[0])\n",
    "print(tdh.training_data_df[['title','language','split']].groupby(['language','split']).count())\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tdh.create_barplot(tdh.training_data_df[tdh.training_data_df.split==split], split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'fr'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], text=False)\n",
    "print('Number of examples for '+language+': ', tdh.training_data_df.shape[0])\n",
    "print(tdh.training_data_df[['title','language','split']].groupby(['language','split']).count())\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tdh.create_barplot(tdh.training_data_df[tdh.training_data_df.split==split], split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'it'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], text=False)\n",
    "print('Number of examples for '+language+': ', tdh.training_data_df.shape[0])\n",
    "print(tdh.training_data_df[['title','language','split']].groupby(['language','split']).count())\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tdh.create_barplot(tdh.training_data_df[tdh.training_data_df.split==split], split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'de,fr'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], text=False)\n",
    "print('Number of examples for '+language+': ', tdh.training_data_df.shape[0])\n",
    "print(tdh.training_data_df[['title','language','split']].groupby(['language','split']).count())\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tdh.create_barplot(tdh.training_data_df[tdh.training_data_df.split==split], split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'de,fr,it'\n",
    "tdh.get_training_data(language=language, affair_text_scope=['zh','ch'], text=False)\n",
    "print('Number of examples for '+language+': ', tdh.training_data_df.shape[0])\n",
    "print(tdh.training_data_df[['title','language','split']].groupby(['language','split']).count())\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tdh.create_barplot(tdh.training_data_df[tdh.training_data_df.split==split], split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdh.training_data_df[['text_de','affair_topic_codes_as_labels','split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdh.filter_training_data(languages='all', affair_text_scope='all', inputs='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdh.raw_data_for_training.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_for_training = pd.read_json('../data/raw_data_for_training.jsonl', lines=True)\n",
    "raw_data_for_training_ch_zh = raw_data_for_training[raw_data_for_training.affair_text_scope.isin(['zh','ch'])]\n",
    "raw_data_for_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_for_training.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The raw training data set has \", raw_data_for_training.shape[0], \" examples.\")\n",
    "print(\"Each affair_text_srcid can have several texts, such as title, Vorstoss etc.\")\n",
    "print(\"If we count only the unique values of affair_text_srcid we get \", raw_data_for_training.drop_duplicates('affair_text_srcid').shape[0], \" examples\")\n",
    "print(raw_data_for_training.drop_duplicates('affair_text_srcid').shape[0], \" is the total number of examples we can work with.\")\n",
    "print(\"If we keep only the data from Zurich and Switzerland (Bund), we have \", raw_data_for_training_ch_zh.drop_duplicates('affair_text_srcid').shape[0], \" examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counting how often each label occurs\n",
    "- First we do a simple count: How often does each label occur per text or row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating overview of the frequency of all labels\n",
    "\n",
    "def create_barplot(df, title=''):\n",
    "\n",
    "    all_labels = list()\n",
    "    for label_list in df.affair_topic_codes_as_labels:\n",
    "        for label in label_list:\n",
    "            all_labels.append(label)\n",
    "\n",
    "    labels_counted = dict(Counter(all_labels))\n",
    "    labels_counted = dict(sorted([x for x in labels_counted.items()], key=lambda x: x[1], reverse=True))\n",
    "    #for label, count in labels_counted.items():\n",
    "        #print(label,': ',count, end=' ; ')\n",
    "    plot_data = pd.DataFrame([labels_counted]) \n",
    "    plot_data = plot_data.sort_values(by=0, ascending=False, axis=1)\n",
    "    ax = plot_data.plot.bar(figsize = (20,15), width=2.2, title=title)\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "        \n",
    "create_barplot(raw_data_for_training,'Frequency of each label per text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, each affair_text_srcid can have several texts, such as title, Vorstoss etc.\n",
    "- We will keep only uniques rows based on affair_text_srcid and then make a count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_barplot(raw_data_for_training.drop_duplicates('affair_text_srcid'),'Frequency of each label per unique affair_text_srcid.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following alos the same plots for the data filtered by ch and zh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_barplot(raw_data_for_training_ch_zh,'Frequency of each label per text only for ch and zh.')\n",
    "create_barplot(raw_data_for_training_ch_zh.drop_duplicates('affair_text_srcid'),'Frequency of each label per unique affair_text_srcid only for ch and zh.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Co-occurence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_for_split_df[list(label2id.keys())].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we need to create a train, validation, test split\n",
    "- According to [this post](https://medium.com/gumgum-tech/creating-balanced-multi-label-datasets-for-model-training-and-evaluation-16b6a3a2d912) this package is good: https://github.com/trent-b/iterative-stratification\n",
    "- Therefore, I will use this python package: https://github.com/trent-b/iterative-stratification\n",
    "- The splits will be done only on the unique affair_text_srcid\n",
    "- We will do the first split only on the basis of the data for *ch* and *zh*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../utils/label2id.json') as f:\n",
    "    label2id = js.load(f)\n",
    "\n",
    "\n",
    "def create_df_for_split(initial_df, colum_for_duplicate_removal='affair_text_srcid'):    \n",
    "    \n",
    "    initial_df = initial_df.drop_duplicates(colum_for_duplicate_removal)\n",
    "\n",
    "\n",
    "    data_for_split = list()\n",
    "\n",
    "    for r in initial_df.to_dict(orient='records'):\n",
    "        item = dict()\n",
    "        affair_text_srcid = r['affair_text_srcid']\n",
    "        affair_topic_codes_as_labels = r['affair_topic_codes_as_labels']\n",
    "        affair_topic_codes = r['affair_topic_codes']\n",
    "        item['affair_text_srcid'] = affair_text_srcid\n",
    "        item['affair_topic_codes_as_labels'] = affair_topic_codes_as_labels\n",
    "        item['affair_topic_codes'] = affair_topic_codes\n",
    "\n",
    "        one_hot_affair_topic_codes = []\n",
    "        for label in sorted(list(label2id.keys())):\n",
    "            if label in affair_topic_codes_as_labels:\n",
    "                item[label]=1\n",
    "                one_hot_affair_topic_codes.append(1)\n",
    "            else:\n",
    "                item[label]=0\n",
    "                one_hot_affair_topic_codes.append(0)\n",
    "\n",
    "        item['one_hot_affair_topic_codes'] = one_hot_affair_topic_codes\n",
    "\n",
    "        data_for_split.append(item)\n",
    "\n",
    "    data_for_split_df = pd.DataFrame(data_for_split)\n",
    "    \n",
    "    data_for_split_df = data_for_split_df.reset_index(drop=True)\n",
    "    \n",
    "    return data_for_split_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_df_for_split(raw_data_for_training_ch_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/trent-b/iterative-stratification\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(['Bla', 'bla', 'bla', 'bla', 'bla', 'bla', 'bla', 'bla'])\n",
    "y = np.array([[0,0,1], [0,0,0], [1,0,1], [1,0,1], [1,1,1], [0,1,1], [1,0,0], [1,0,0]])\n",
    "\n",
    "\n",
    "\n",
    "def create_split(dataframe, test_size=0.4):\n",
    "    \n",
    "    if 'split' not in dataframe.columns:\n",
    "        dataframe['split']=''\n",
    "    \n",
    "    X = dataframe.affair_text_srcid.values\n",
    "    y = np.array(dataframe.one_hot_affair_topic_codes.tolist())\n",
    "    \n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=0)\n",
    "\n",
    "    for train_index, test_index in msss.split(X, y):\n",
    "        dataframe.loc[(dataframe.index.isin(train_index)), 'split'] = 'train'\n",
    "        dataframe.loc[(dataframe.index.isin(test_index)), 'split'] = 'other'\n",
    "        \n",
    "        dataframe_filtered = dataframe[dataframe.split=='other'].reset_index(drop=False)\n",
    "        X = dataframe_filtered.affair_text_srcid.values\n",
    "        y = np.array(dataframe_filtered.one_hot_affair_topic_codes.tolist())\n",
    "        \n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "\n",
    "        for validation_index, test_index in msss.split(X, y):\n",
    "            for i,_ in dataframe_filtered.iterrows():\n",
    "                _index = dataframe_filtered.at[i, 'index']\n",
    "                if i in validation_index:\n",
    "                    dataframe.at[_index,'split']='validation'\n",
    "                if i in test_index:\n",
    "                    dataframe.at[_index,'split']='test'\n",
    "            \n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_split_df = create_df_for_split(raw_data_for_training_ch_zh)\n",
    "\n",
    "data_for_split_df = create_split(data_for_split_df)\n",
    "data_for_split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_split_df[['affair_text_srcid', 'split']].groupby('split').count().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if the labels have an equal distribution over all splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    train_affair_text_srcid = data_for_split_df[data_for_split_df.split==split].affair_text_srcid.tolist()\n",
    "    train_affair_text_srcid = [int(x) for x in train_affair_text_srcid]\n",
    "    create_barplot(raw_data_for_training_ch_zh_unique[(raw_data_for_training_ch_zh_unique.affair_text_srcid.isin(train_affair_text_srcid))], title=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer, sample_dataset\n",
    "\n",
    "\n",
    "# Load a dataset from the Hugging Face Hub\n",
    "# dataset = load_dataset(\"sst2\")\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"joelito/lextreme\", \"covid19_emergency_event\")\n",
    "\n",
    "# Simulate the few-shot regime by sampling 8 examples per class\n",
    "train_dataset = dataset[\"train\"] #[:20]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "train_dataset = train_dataset.remove_columns('language')\n",
    "eval_dataset = eval_dataset.remove_columns('language')\n",
    "\n",
    "# Load a SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    multi_target_strategy=\"one-vs-rest\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=16,\n",
    "    num_iterations=20, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1, # The number of epochs to use for contrastive learning\n",
    "    column_mapping={\"input\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# Push model to the Hub\n",
    "trainer.push_to_hub(\"my-awesome-setfit-model\")\n",
    "\n",
    "# Download from Hub and run inference\n",
    "model = SetFitModel.from_pretrained(\"lewtun/my-awesome-setfit-model\")\n",
    "# Run inference\n",
    "preds = model([\"i loved the spiderman movie!\", \"pineapple on pizza is the worst ðŸ¤®\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] #.features['label'].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/Data/affairs.xlsx')\n",
    "#df = df[df.affair_scope==\"zh\"]\n",
    "df = df[df.affair_topic_codes.isin(['1696','other'])]\n",
    "df = df[df.affair_topic_codes.isnull()==False]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label2id = dict()\n",
    "\n",
    "topic_labels = pd.read_excel('../data/topics_politmonitor.xlsx')\n",
    "\n",
    "for r in topic_labels.to_dict(orient='records'):\n",
    "    item = dict()\n",
    "    label2id[r[\"keyword_de\"]]=int(r['keyword_id'])\n",
    "    \n",
    "for label_list in df.affair_topic_codes.tolist():\n",
    "    for label in label_list:\n",
    "        if int(label) not in label2id.values():\n",
    "            label2id[int(label)]='not_specified_label_'+str(label)\n",
    "    \n",
    "with open('../data/label2id.json','w') as f:\n",
    "    js.dump(label2id, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_topic_codes(affair_topic_codes):\n",
    "    affair_topic_codes_as_list = affair_topic_codes.split(';')\n",
    "    affair_topic_codes_as_list = [int(x) for x in affair_topic_codes_as_list if x]\n",
    "    \n",
    "    return sorted(affair_topic_codes_as_list)\n",
    "   \n",
    "df = df.fillna('')\n",
    "df[\"affair_topic_codes\"]=df.affair_topic_codes.apply(process_topic_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = list()\n",
    "X = df.title_de.to_numpy()\n",
    "y = df.affair_topic_codes.to_numpy()\n",
    "for r in df.to_dict(orient=\"records\"):\n",
    "    if len(r['affair_topic_codes'])>0:\n",
    "        item = dict()\n",
    "        item['input']=r['title_de']\n",
    "        labels_as_string = list()\n",
    "        for label, _id in label2id.items():\n",
    "            if _id in r['affair_topic_codes']:\n",
    "                item[label]=1\n",
    "                #labels_as_string.append(str(_id))\n",
    "            else:\n",
    "                item[label]=0\n",
    "                #labels_as_string.append(str('None'))\n",
    "        item['labels_as_string']=';'.join([str(x) for x in r['affair_topic_codes']])\n",
    "        item['label']=r['affair_topic_codes']\n",
    "        dataset.append(item)\n",
    "    \n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset['language']='de'\n",
    "dataset.head()\n",
    "\n",
    "frequency_count = Counter(dataset.labels_as_string)\n",
    "\n",
    "# StratifiedShuffleSplit cannot process labels that occur only once\n",
    "# Therefore, for all labels that occur only once we give the value other in the field labels_as_string\n",
    "for i, _ in dataset.iterrows():\n",
    "    labels_as_string = dataset.at[i, 'labels_as_string']\n",
    "    if frequency_count[labels_as_string]<5:\n",
    "        dataset.at[i, 'labels_as_string']='other'\n",
    "\n",
    "frequency_count = Counter(dataset.labels_as_string)\n",
    "frequency_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = dataset.input\n",
    "y = dataset.labels_as_string\n",
    "\n",
    "\n",
    "split_indices = dict()\n",
    "\n",
    "X_train, X_1, y_train, y_1 = train_test_split(X, y, test_size=0.4, random_state=42, shuffle=True)\n",
    "\n",
    "split_indices['train']=X_train.index\n",
    "    \n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_1, y_1, test_size=0.5, random_state=42, shuffle=True)\n",
    "split_indices['validation']=X_validation.index\n",
    "split_indices['test']=X_test.index\n",
    "\n",
    "dataset['split']=''\n",
    "for i, _ in dataset.iterrows():\n",
    "    if i in split_indices['train']:\n",
    "        dataset.at[i,'split']='train'\n",
    "    if i in split_indices['validation']:\n",
    "        dataset.at[i,'split']='validation'\n",
    "    if i in split_indices['test']:\n",
    "        dataset.at[i,'split']='test'\n",
    "    #else:\n",
    "        #dataset.at[i,'split']='train'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['input','split']].groupby('split').count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.split=='train'][['input','labels_as_string']].groupby('labels_as_string').count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.split=='validation'][['input','labels_as_string']].groupby('labels_as_string').count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.split=='test'][['input','labels_as_string']].groupby('labels_as_string').count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_json('../data/dataset_for_training.jsonl',lines=True, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetDict()\n",
    "\n",
    "for split in ['train','validation','test']:\n",
    "    x = Dataset.from_pandas(dataset[dataset.split==split][['input','label','language']])\n",
    "    ds[split] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('../data/dataset_for_training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
