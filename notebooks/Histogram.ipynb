{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenization_length(tokenizer, text):\n",
    "    if type(text)==str:\n",
    "        result = tokenizer(text,add_special_tokens=False, return_length=True)\n",
    "    if type(text) in (list, tuple):\n",
    "        if len(text)==0:\n",
    "            result = dict()\n",
    "            result['length']=0\n",
    "        else:\n",
    "            text = [str(x) for x in text]\n",
    "            try:\n",
    "                result = tokenizer(text, is_split_into_words=True,add_special_tokens=False, return_length=True)\n",
    "            except:\n",
    "                print(text)\n",
    "    else:\n",
    "        text = str(text)\n",
    "    if type(result['length'])==int:\n",
    "        return result['length']\n",
    "    elif type(result['length'])==list:\n",
    "        return result['length'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_be_used = [\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"microsoft/Multilingual-MiniLM-L12-H384\",\n",
    "    \"microsoft/mdeberta-v3-base\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlm-roberta-large\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lextreme_datasets = ['brazilian_court_decisions_judgment', 'brazilian_court_decisions_unanimity', \n",
    "                     'swiss_judgment_prediction', 'german_argument_mining', 'greek_legal_code_volume', \n",
    "                     'greek_legal_code_chapter', 'greek_legal_code_subject', \n",
    "                     'online_terms_of_service_unfairness_levels', 'online_terms_of_service_clause_topics', \n",
    "                     'covid19_emergency_event', 'multi_eurlex_level_1', 'lener_br', 'legalnero', \n",
    "                     'greek_legal_ner', 'mapa_coarse', 'mapa_fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_languages(dataset):\n",
    "    dataset_new = list()\n",
    "    \n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    \n",
    "    for item in dataset_df.to_dict(orient='records'):\n",
    "        labels = item['label']\n",
    "        for language, document in literal_eval(item['input']).items():\n",
    "            if document is not None:\n",
    "                item_new = dict()\n",
    "                item_new['language']=language\n",
    "                item_new['input']=str(document)\n",
    "                item_new['label']=labels\n",
    "                dataset_new.append(item_new)\n",
    "    \n",
    "    dataset_new = pd.DataFrame(dataset_new)\n",
    "    \n",
    "    \n",
    "    return  dataset_new\n",
    "\n",
    "def generate_historgram(dataset_name, dataframe,language=None):\n",
    "    \n",
    "    if language is None:\n",
    "        all_data_as_df_filtered = dataframe\n",
    "    else:\n",
    "        all_data_as_df_filtered = dataframe[dataframe.language==language]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    \n",
    "    plots = list()\n",
    "    labels = list()\n",
    "    for lmt in set(models_to_be_used):\n",
    "        bins = np.linspace(0, max(list(all_data_as_df_filtered[lmt])),100)\n",
    "        plots.append(list(all_data_as_df_filtered[lmt]))\n",
    "        labels.append(lmt)\n",
    "    \n",
    "    plt.hist(plots, bins, alpha=0.5, label=labels)\n",
    "    \n",
    "    \n",
    "    if dataset_name=='multi_eurlex_level_1':   \n",
    "        plt.xlim(0, 30000)\n",
    "    elif dataset_name.startswith('greek_legal_code'):\n",
    "        plt.xlim(0, 20000)\n",
    "        \n",
    "\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "    if language is None:\n",
    "        plt.xlabel('Length of input', fontsize=16)\n",
    "    else:\n",
    "        plt.xlabel('Length of input for language '+language, fontsize=16)\n",
    "    plt.ylabel('Frequency of length value', fontsize=16)\n",
    "    plt.title(dataset_name, fontsize=16)\n",
    "    \n",
    "    if language is None:\n",
    "        plt.savefig('../figures/'+dataset_name+'/histogram_'+'_'.join(dataset_name.split())+'.jpg')\n",
    "        \n",
    "    else:\n",
    "        plt.savefig('../figures/'+dataset_name+'/histogram_'+'_'.join(dataset_name.split())+'__'+language+'.jpg')\n",
    "        \n",
    "    #plt.clf()\n",
    "        \n",
    "\n",
    "def create_histograms(dataset_name, language='all'):\n",
    "    \n",
    "    if Path('../figures/'+dataset_name).exists():\n",
    "        shutil.rmtree('../figures/'+dataset_name)\n",
    "        os.mkdir('../figures/'+dataset_name)\n",
    "    else:\n",
    "        os.mkdir('../figures/'+dataset_name)\n",
    "        \n",
    "    \n",
    "    dataset = load_dataset(\"joelito/lextreme\",dataset_name, download_mode=\"force_redownload\") \n",
    "\n",
    "    all_data_as_df = list()\n",
    "\n",
    "    for split in ['train','validation','test']:\n",
    "        df = pd.DataFrame(dataset[split])\n",
    "        all_data_as_df.append(df)\n",
    "        \n",
    "\n",
    "    all_data_as_df = pd.concat(all_data_as_df)\n",
    "    all_data_as_df = all_data_as_df\n",
    "    \n",
    "    if dataset_name.startswith('multi_eurlex'):\n",
    "        all_data_as_df = split_into_languages(all_data_as_df)\n",
    "            \n",
    "    for lmt in models_to_be_used:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(lmt)\n",
    "        all_data_as_df[lmt]=all_data_as_df.input.apply(lambda x: get_tokenization_length(tokenizer,x))\n",
    "    \n",
    "    if language == 'all':\n",
    "        \n",
    "        if 'language' in all_data_as_df.columns.tolist():\n",
    "            \n",
    "            for lang in all_data_as_df.language.unique():\n",
    "            \n",
    "                generate_historgram(dataset_name,all_data_as_df,lang)\n",
    "        \n",
    "        if len(all_data_as_df.language.unique())>1:\n",
    "            \n",
    "            generate_historgram(dataset_name,all_data_as_df)\n",
    "            \n",
    "    else:\n",
    "        generate_historgram(dataset_name,all_data_as_df,language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ds in lextreme_datasets:\n",
    "    #create_histograms(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/greek_legal_code_chapter/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n",
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/online_terms_of_service_unfairness_levels/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n",
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/greek_legal_code_subject/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n",
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/greek_legal_ner/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/greek_legal_code_volume/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n",
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/mapa_coarse/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/brazilian_court_decisions_unanimity/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n",
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/lener_br/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/covid19_emergency_event/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset lextreme/multi_eurlex_level_1 to /home/veton/.cache/huggingface/datasets/joelito___lextreme/multi_eurlex_level_1/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/brazilian_court_decisions_judgment/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/legalnero/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/swiss_judgment_prediction/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset lextreme/mapa_fine to /home/veton/.cache/huggingface/datasets/joelito___lextreme/mapa_fine/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/online_terms_of_service_clause_topics/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (/home/veton/.cache/huggingface/datasets/joelito___lextreme/german_argument_mining/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/veton/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration joelito--mapa-249f169929eac49d\n",
      "Reusing dataset json (/home/veton/.cache/huggingface/datasets/joelito___json/joelito--mapa-249f169929eac49d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3295 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3295 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3295 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/veton/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/veton/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "<ipython-input-5-d25444d847cd>:29: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(20,10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lextreme downloaded and prepared to /home/veton/.cache/huggingface/datasets/joelito___lextreme/multi_eurlex_level_1/1.0.0/f90df74c0cbea90e83c25c7e114c81fe3d2b16bae1afc96558b870e8f99beb11. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool() as p:\n",
    "    print(p.map(create_histograms, lextreme_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
