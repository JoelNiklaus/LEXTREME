{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import recall_score\n",
    "from seqeval.metrics import precision_score\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (C:\\Users\\vmatoshi\\.cache\\huggingface\\datasets\\joelito___lextreme\\greek_legal_ner\\1.0.0\\14697fe76aebeca2a6a3c25166bd6eb61f769e4ad7a19bd520634691a4f70bc5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285c867a37bd40de97700fa57f790491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lextreme (C:\\Users\\vmatoshi\\.cache\\huggingface\\datasets\\joelito___lextreme\\lener_br\\1.0.0\\14697fe76aebeca2a6a3c25166bd6eb61f769e4ad7a19bd520634691a4f70bc5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c3ca74d4ed4c67b749213603fc6b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "greek_legal_ner = load_dataset(\"joelito/lextreme\", \"greek_legal_ner\")\n",
    "lener_br = load_dataset(\"joelito/lextreme\",\"lener_br\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "- There seems to exist a bunch of ways of how to evaluate the results of NER\n",
    "    - \n",
    "    \n",
    "- There are two major problems:\n",
    "    - 1. We need to decide which method we are going to use. \n",
    "    - 2. In many papers people don't really state how they calculated the scores. In those cases, we need to read their code and figure out how they did it. In other cases, they explained it very well, as in this paper:\n",
    "        - Angelidis, I., Chalkidis, I., & Koubarakis, M. (2018). Named Entity Recognition, Linking and Generation for Greek Legislation. JURIX.\n",
    "          - *\\\"MUC guidelines consider partial token overlaps between the gold annotations and the predicted entities(sequences of consecutive tokens that belong in the same class), given the correct (gold) class.\\\"*\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How we did it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We used the library seqeval: https://github.com/chakki-works/seqeval\n",
    "- The documentation states that seqeval supports the following schemes:\n",
    "    IOB1\n",
    "    IOB2\n",
    "    IOE1\n",
    "    IOE2\n",
    "    IOBES(only in strict mode)\n",
    "    BILOU(only in strict mode)\n",
    "- So, usually, when you annotate NER tags, use specify when a NER tag starts and ends. This is usally done be some sort of prefix, as I- = inside or O- = outside.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Problem\n",
    "\n",
    "- The LEXTREME dataset contains a lot of NER datasets\n",
    "- But those are not presented using the same scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All NER datasets that we created follow a very simple scheme, that is not supported by seqeval, which means that the label column only gives the ID of the label without specifying whether it is the beginning or the end of a tag\n",
    "- So we have these label classes:\n",
    "\n",
    "\"label_classes\": [\n",
    "        \"FACILITY\",\n",
    "        \"GPE\",\n",
    "        \"LEG-REFS\",\n",
    "        \"LOCATION-NAT\",\n",
    "        \"LOCATION-UNK\",\n",
    "        \"ORG\",\n",
    "        \"PERSON\",\n",
    "        \"PUBLIC-DOCS\",\n",
    "        \"O\",\n",
    "    ]\n",
    "       - NOTE that, if we use this kind of annotation, we must be very sure that the two instances of the same named entity do not occur adjacently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ΝΟΜΟΣ, ΥΠ', ΑΡΙΘΜ, ., 4156]</td>\n",
       "      <td>[8, 8, 8, 8, 8]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Μνημόνιο, Κατανόησης, στον, τομέα, της, προστ...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ο, ΠΡΟΕΔΡΟΣ, ΤΗΣ, ΕΛΛΗΝΙΚΗΣ, ΔΗΜΟΚΡΑΤΙΑΣ]</td>\n",
       "      <td>[8, 8, 8, 1, 1]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Εκδίδομε, τον, ακόλουθο, νόμο, που, ψήφισε, η...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 5, 8]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Άρθρο, 1]</td>\n",
       "      <td>[8, 8]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17694</th>\n",
       "      <td>[Την, έγκριση, της, πρότασης, της, Κοινότητας,...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17695</th>\n",
       "      <td>[5.000, ., Κατά, της, παρούσης, επιτρέπεται, η...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17696</th>\n",
       "      <td>[Η, απόφαση, αυτή, να, δημοσιευθεί, στην, Εφημ...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17697</th>\n",
       "      <td>[Παλλήνη, ,, 1, Δεκεμβρίου, 2004, Ο, Νομάρχης,...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 6]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17698</th>\n",
       "      <td>[ΚΟΥΡΗΣ]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17699 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0                           [ΝΟΜΟΣ, ΥΠ', ΑΡΙΘΜ, ., 4156]   \n",
       "1      [Μνημόνιο, Κατανόησης, στον, τομέα, της, προστ...   \n",
       "2             [Ο, ΠΡΟΕΔΡΟΣ, ΤΗΣ, ΕΛΛΗΝΙΚΗΣ, ΔΗΜΟΚΡΑΤΙΑΣ]   \n",
       "3      [Εκδίδομε, τον, ακόλουθο, νόμο, που, ψήφισε, η...   \n",
       "4                                             [Άρθρο, 1]   \n",
       "...                                                  ...   \n",
       "17694  [Την, έγκριση, της, πρότασης, της, Κοινότητας,...   \n",
       "17695  [5.000, ., Κατά, της, παρούσης, επιτρέπεται, η...   \n",
       "17696  [Η, απόφαση, αυτή, να, δημοσιευθεί, στην, Εφημ...   \n",
       "17697  [Παλλήνη, ,, 1, Δεκεμβρίου, 2004, Ο, Νομάρχης,...   \n",
       "17698                                           [ΚΟΥΡΗΣ]   \n",
       "\n",
       "                                                   label language  \n",
       "0                                        [8, 8, 8, 8, 8]       el  \n",
       "1      [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...       el  \n",
       "2                                        [8, 8, 8, 1, 1]       el  \n",
       "3                            [8, 8, 8, 8, 8, 8, 8, 5, 8]       el  \n",
       "4                                                 [8, 8]       el  \n",
       "...                                                  ...      ...  \n",
       "17694  [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...       el  \n",
       "17695  [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...       el  \n",
       "17696                     [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]       el  \n",
       "17697                           [8, 8, 8, 8, 8, 8, 8, 6]       el  \n",
       "17698                                                [6]       el  \n",
       "\n",
       "[17699 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(greek_legal_ner['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the other side, we have lener_br that uses a scheme supported by seqeval\n",
    "- So, we have these label classes:\n",
    "\n",
    "\"label_classes\": [\n",
    "        \"O\",\n",
    "        \"B-ORGANIZACAO\",\n",
    "        \"I-ORGANIZACAO\",\n",
    "        \"B-PESSOA\",\n",
    "        \"I-PESSOA\",\n",
    "        \"B-TEMPO\",\n",
    "        \"I-TEMPO\",\n",
    "        \"B-LOCAL\",\n",
    "        \"I-LOCAL\",\n",
    "        \"B-LEGISLACAO\",\n",
    "        \"I-LEGISLACAO\",\n",
    "        \"B-JURISPRUDENCIA\",\n",
    "        \"I-JURISPRUDENCIA\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EMENTA, :, APELAÇÃO, CÍVEL, -, AÇÃO, DE, INDE...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-, O, art, ., 178, ,, II, ,, do, CPC, prescre...</td>\n",
       "      <td>[0, 0, 9, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0,...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-, Tratando-se, de, ação, indenizatória, ajui...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-, Tendo, o, vício, sido, arguido, pelo, Parq...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-, Preliminar, acolhida, para, reconhecer, a,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7823</th>\n",
       "      <td>[Verbas, Rescisórias, .]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824</th>\n",
       "      <td>[Controversas, '', ,, por, ofensa, ao, artigo,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 9, 10, 10, 10, 0, 0, 0, 0, ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7825</th>\n",
       "      <td>[Brasília, ,, 04, de, novembro, de, 2015, .]</td>\n",
       "      <td>[7, 0, 5, 6, 6, 6, 6, 0]</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>[Firmado, por, assinatura, digital, (, MP, 2.2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 9, 10, 0, 3, 4, 4, 4, 0, 0, 0,...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7828 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "0     [EMENTA, :, APELAÇÃO, CÍVEL, -, AÇÃO, DE, INDE...   \n",
       "1     [-, O, art, ., 178, ,, II, ,, do, CPC, prescre...   \n",
       "2     [-, Tratando-se, de, ação, indenizatória, ajui...   \n",
       "3     [-, Tendo, o, vício, sido, arguido, pelo, Parq...   \n",
       "4     [-, Preliminar, acolhida, para, reconhecer, a,...   \n",
       "...                                                 ...   \n",
       "7823                           [Verbas, Rescisórias, .]   \n",
       "7824  [Controversas, '', ,, por, ofensa, ao, artigo,...   \n",
       "7825       [Brasília, ,, 04, de, novembro, de, 2015, .]   \n",
       "7826  [Firmado, por, assinatura, digital, (, MP, 2.2...   \n",
       "7827                                                 []   \n",
       "\n",
       "                                                  label language  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       pt  \n",
       "1     [0, 0, 9, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0,...       pt  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       pt  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       pt  \n",
       "4                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]       pt  \n",
       "...                                                 ...      ...  \n",
       "7823                                          [0, 0, 0]       pt  \n",
       "7824  [0, 0, 0, 0, 0, 0, 9, 10, 10, 10, 0, 0, 0, 0, ...       pt  \n",
       "7825                           [7, 0, 5, 6, 6, 6, 6, 0]       pt  \n",
       "7826  [0, 0, 0, 0, 0, 9, 10, 0, 3, 4, 4, 4, 0, 0, 0,...       pt  \n",
       "7827                                                 []       pt  \n",
       "\n",
       "[7828 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lener_br['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It makes a difference concerning the question what is perceived as correct and not correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Let's take, for example, the scheme used in greek_legal_ner**\n",
    "- We add the prefix I-, because seqeval truncates the tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, everything is perfectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1:  1.0\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, instead of PER, O is predicted\n",
    "- As we can see, accuracy is measured on token level\n",
    "- All the other scores are calculated on NER level\n",
    "- There are two instaced of PER; since the model missed one single token for the second instance of PER, the second instance of PER is not included in the cases of correctly classified NERs at all. This is a big difference in comparison to the scheme used in Angelidis, I., Chalkidis, I., & Koubarakis, M. (2018). Named Entity Recognition, Linking and Generation for Greek Legislation. JURIX.\n",
    "  - In other word: while the scores for MISC remain perfect, the score for f1, precision and recall is 0.50, because half of the instances of PER where not detected (even though only one token was not detected correctly) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It does not matter, if another token of the same NE instance is wrong, except for the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','O','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exception: if the entire NE is not detected at all (i.e. everything is O), it does not count, which is why recall stays the same, but precision increases again yield a better F1 score \n",
    "  - **this is weird**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n",
      "Precision:  1.0\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.8\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       1.00      0.67      0.80         3\n",
      "   macro avg       1.00      0.75      0.83         3\n",
      "weighted avg       1.00      0.67      0.78         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','O','O','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following the results, if we have false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','O','O']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','I-PER','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','O','O']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','I-PER','I-PER']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Let's take, for example, the scheme used in lener_br**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1:  1.0\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.6666666666666666\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','O','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again, if the entire NE is not detected at all (i.e. everything is O), it does not count, which is why recall stays the same, but precision increases again yield a better F1 score \n",
    "  - **this is weird**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n",
      "Precision:  1.0\n",
      "Recall:  0.6666666666666666\n",
      "F1:  0.8\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       1.00      0.67      0.80         3\n",
      "   macro avg       1.00      0.75      0.83         3\n",
      "weighted avg       1.00      0.67      0.78         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O','B-PER','I-PER','I-PER']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O','O','O','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following the results, if we have false positives\n",
    "     - The results are slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9\n",
      "Precision:  0.75\n",
      "Recall:  1.0\n",
      "F1:  0.8571428571428571\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.67      1.00      0.80         2\n",
      "\n",
      "   micro avg       0.75      1.00      0.86         3\n",
      "   macro avg       0.83      1.00      0.90         3\n",
      "weighted avg       0.78      1.00      0.87         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','O','O']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','B-PER','O']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "Precision:  0.75\n",
      "Recall:  1.0\n",
      "F1:  0.8571428571428571\n",
      "------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       0.67      1.00      0.80         2\n",
      "\n",
      "   micro avg       0.75      1.00      0.86         3\n",
      "   macro avg       0.83      1.00      0.90         3\n",
      "weighted avg       0.78      1.00      0.87         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','O','O']]\n",
    "y_pred = [['B-PER', 'I-PER', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'B-PER','I-PER','B-PER','I-PER']]\n",
    "print('Accuracy: ',accuracy_score(y_true, y_pred))\n",
    "print('Precision: ',precision_score(y_true, y_pred))\n",
    "print('Recall: ',recall_score(y_true, y_pred))\n",
    "print('F1: ',f1_score(y_true, y_pred))\n",
    "print('------------------------')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I think, we can stick to seqeval, but we must be able to understands it internal functioning\n",
    "- We should definitly use the same scheme for all our NER datasets!\n",
    "- It might be necessary to provide four different metrics, as it was done in this paper:\n",
    "    - <div class=\"csl-entry\">Segura-Bedmar, I., Mart\\’\\inez, P., &#38; Herrero-Zazo, M. (2013). SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013). <i>Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)</i>, 341–350. https://aclanthology.org/S13-2056</div>\n",
    "    \n",
    "     - *Our evaluation metricsshould score if a system is able to identify the ex-act span of an entity (regardless of the type) and ifit is able to assign the correct entity type (regardlessof the boundaries). Thus, our evaluation script willoutput four sets of scores according to*:\n",
    "      - 1. Strict evaluation (exact-boundary and typematching).\n",
    "      - 2. Exact boundary matching (regardless to thetype).\n",
    "      - 3. Partial boundary matching (regardless to thetype).\n",
    "      - 4. Type matching (some overlap between thetagged entity and the gold entitity is required).\n",
    "      \n",
    "\n",
    "- Maybe one of these libraries might help us:\n",
    " - https://github.com/MantisAI/nervaluate\n",
    " - If we do an evluation on token level, we might also just use scikit learn by treating each token as a sample\n",
    "     - https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
