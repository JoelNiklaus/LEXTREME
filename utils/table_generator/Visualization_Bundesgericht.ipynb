{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02283429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_overview import ResultAggregator\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from helper import *\n",
    "import json as js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608daa08",
   "metadata": {},
   "source": [
    "# 1. Starting\n",
    "\n",
    "- There are two ways to start the code\n",
    " - You can either directly fetch the data from the project from [wandb](https://wandb.ai/site) or\n",
    " - you can read the data from a pre-existing csv file\n",
    "- Note: Whenever you fetch the data directly from wandb, the data is automatically stored in a csv file called ```current_wandb_results_unprocessed.csv```. This measurement is taken, because downloading the data from wandb can take a while and sometimes even be interrupted due to connection issues (```Read timed out.``` etc.). Therefore, having a csv file can speed up the data reading.\n",
    "- Initializing the object can happen in two ways, depending on whether you want to fetch the data from wandb or whether you want to read it from an existing csv file\n",
    "- To fetch the data directly from wand to you need to pass a valid wand api key:\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(wandb_api_key={WAND_API_KEY})\n",
    "\n",
    "```\n",
    "- To use the data from an existing csv file, you need to pass the path to the csv file (in the example below, the path to the automatically generated csv file is given):\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(path_to_csv_export=\"current_wandb_results_unprocessed.csv\")\n",
    "\n",
    "```\n",
    "\n",
    "- Optionally, you can set the argument ```verbose_logging```to ```False```. This might be usefule, to avoid overwhelming logging messages either in the terminal or in a notebook cell. The logs are saved in a log file in the log directory anyway.\n",
    "- Passing the argument ```verbose_logging``` can be done like this:\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(wandb_api_key={WAND_API_KEY}, verbose_logging=False)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(path_to_csv_export=\"current_wandb_results_unprocessed.csv\", verbose_logging=False)\n",
    "\n",
    "```\n",
    "\n",
    "- It is important to note that all overviews and aggregates evolves around the score `macro-f1`. But you can overwrite this by passing `score` with the name of the score you are interested in, like this:\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(path_to_csv_export=\"current_wandb_results_unprocessed.csv\", score='micro-f1')\n",
    "\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f4a1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_specs = 'multi_legal_pile_only_lextreme'\n",
    "with open(TABLE_TO_REPORT_SPECS + f\"/{report_specs}.json\", 'r') as f:\n",
    "    report_spec = js.load(f)\n",
    "\n",
    "list_of_seeds = [1,2,3]\n",
    "\n",
    "model_constraint = [x.split('@')[0] for x in report_spec['_name_or_path']]\n",
    "task_constraint = report_spec['finetuning_task']\n",
    "\n",
    "ra = ResultAggregator(\n",
    "                          project_name=report_spec['wandb_project_name'],\n",
    "                          output_dir=TABLE_OUTPUT_PATH + f'/{report_spec[\"wandb_project_name\"]}',\n",
    "                          path_to_csv_export=None,\n",
    "                          verbose_logging=False,\n",
    "                          only_completed_tasks=False,\n",
    "                          which_language=None,\n",
    "                          required_seeds=list_of_seeds,\n",
    "                          report_spec_name='multi_legal_pile_only_lextreme',\n",
    "                          fill_with_wrong_revisions=False,\n",
    "                          mean_type='harmonic',\n",
    "                          remove_outliers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d91b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra.meta_infos['model_language_lookup_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.create_overview_of_results_per_seed(score=\"macro-f1\",only_completed_tasks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c028343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_mapping = dict()\n",
    "for model_type, infos in ra.meta_infos['language_models'].items():\n",
    "    for language, size_and_models in infos.items():\n",
    "        for size, models in size_and_models.items():\n",
    "            for m in models:\n",
    "                model_type_mapping[m]=model_type\n",
    "set(model_type_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa453dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra.meta_infos['task_type_mapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d9b27",
   "metadata": {},
   "source": [
    "# Visualizations for the Bundesgericht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5852235",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.meta_infos['dataset_abbreviations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d32831",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_task = pd.DataFrame([{'Type': v, 'Task': k} for k,v in ra.meta_infos['task_type_mapping'].items()]) \n",
    "type_task['Dataset'] = type_task.Task.apply(lambda x: ra.meta_infos['config_to_dataset'][x])\n",
    "type_task['Dataset Abb.'] = type_task.Dataset.apply(lambda x: ra.meta_infos[\"dataset_abbreviations\"][x])\n",
    "type_task = type_task[['Type','Dataset','Dataset Abb.','Task']]\n",
    "type_task.to_excel('/Users/vetonmatoshi/Downloads/figures_joel/task_types.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_models = [k for k,v in ra.meta_infos['model_language_lookup_table'].items() if v=='all' and k in model_constraint]\n",
    "monolingual_models = [k for k,v in ra.meta_infos['model_language_lookup_table'].items() if v!='all' and k in model_constraint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(data, color='lawngreen'):\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    data = data.astype(float)\n",
    "    if data.ndim == 1:  \n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    else: \n",
    "        is_max = data == data.max().max()\n",
    "        return pd.DataFrame(np.where(is_max, attr, ''),\n",
    "                            index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102530c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def round_values(value):\n",
    "    if type(value)!=str:\n",
    "        try:\n",
    "            return literal_eval('{:.2f}'.format(round(value*100, 2)))\n",
    "        except:\n",
    "            return value\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.get_dataset_aggregated_score(average_over_language=True, model_constraint=model_constraint, task_constraint=task_constraint)\n",
    "dataset_agg_score_multilingual_models = ra.dataset_aggregated_score.loc[multilingual_models]\n",
    "dataset_agg_score_multilingual_models = dataset_agg_score_multilingual_models.applymap(round_values)\n",
    "dataset_agg_score_multilingual_models\n",
    "dataset_agg_score_multilingual_models = dataset_agg_score_multilingual_models.sort_values('Agg.', ascending=False).style.apply(highlight_max)\n",
    "dataset_agg_score_multilingual_models.to_excel(\"/Users/vetonmatoshi/Downloads/figures_joel/dataset_agg_score_multilingual_models.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b97f1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "ra.get_language_aggregated_score()\n",
    "language_agg_score_multilingual_models = ra.language_aggregated_score.loc[multilingual_models]\n",
    "# del language_agg_score_multilingual_models['tr']\n",
    "language_agg_score_multilingual_models = language_agg_score_multilingual_models.applymap(round_values)\n",
    "language_agg_score_multilingual_models = language_agg_score_multilingual_models.sort_values('aggregated_score', ascending=False) #.style.highlight_max(color = 'lightgreen', axis = 0)\n",
    "language_agg_score_multilingual_models = language_agg_score_multilingual_models.style.apply(highlight_max)\n",
    "\n",
    "language_agg_score_multilingual_models.to_excel(\"/Users/vetonmatoshi/Downloads/figures_joel/language_agg_score_multilingual_models.xlsx\", index=True)\n",
    "language_agg_score_multilingual_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f626611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "ra.get_language_aggregated_score()\n",
    "language_agg_score_monolingual_models = ra.language_aggregated_score.loc[monolingual_models]\n",
    "del language_agg_score_monolingual_models['tr']\n",
    "language_agg_score_monolingual_models = language_agg_score_monolingual_models.applymap(round_values)\n",
    "language_agg_score_monolingual_models = language_agg_score_monolingual_models.sort_values('aggregated_score', ascending=False) #.style.highlight_max(color = 'lightgreen', axis = 0)\n",
    "language_agg_score_monolingual_models = language_agg_score_monolingual_models[language_agg_score_monolingual_models.aggregated_score.apply(lambda x: x!='nan')]\n",
    "\n",
    "language_agg_score_monolingual_models['language'] = language_agg_score_monolingual_models.index.map(lambda x: ra.meta_infos['model_language_lookup_table'][x])\n",
    "language_agg_score_monolingual_models = language_agg_score_monolingual_models[['language','aggregated_score']]\n",
    "language_agg_score_monolingual_models = language_agg_score_monolingual_models.sort_values(['language','aggregated_score'], ascending=False)\n",
    "language_agg_score_monolingual_models.to_excel(\"/Users/vetonmatoshi/Downloads/figures_joel/language_agg_score_monolingual_models.xlsx\", index=True)\n",
    "#language_agg_score_monolingual_models = language_agg_score_monolingual_models.style.apply(highlight_max)\n",
    "language_agg_score_monolingual_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a00e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1 = ra.create_overview_of_results_per_seed(score=\"macro-f1\",only_completed_tasks=True)\n",
    "macro_f1 = macro_f1[(macro_f1.finetuning_task=='swiss_judgment_prediction') & (macro_f1.language.isin(['de','fr','it'])) & (macro_f1._name_or_path.isin(multilingual_models)==False)]\n",
    "macro_f1 = macro_f1.applymap(round_values)\n",
    "macro_f1 = macro_f1.sort_values(['language','mean_over_seeds'], ascending=False)\n",
    "macro_f1.to_excel(\"/Users/vetonmatoshi/Downloads/figures_joel/sjp_mean_overseeds.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.get_language_aggregated_score()\n",
    "df = ra.language_aggregated_score.loc[monolingual_models]\n",
    "del df['tr']\n",
    "df['_name_or_path'] = df.index.tolist()\n",
    "mean_joels_models = df[df._name_or_path.str.contains('joelito')].aggregated_score.mean()\n",
    "mean_not_joels_models = df[df._name_or_path.str.contains('joelito')==False].aggregated_score.mean()\n",
    "print(mean_joels_models)\n",
    "print(mean_not_joels_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e50a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1 = ra.create_overview_of_results_per_seed(score=\"macro-f1\",only_completed_tasks=True)\n",
    "macro_f1 = macro_f1[macro_f1.language!='all']\n",
    "macro_f1 = macro_f1[macro_f1._name_or_path.isin(monolingual_models)]\n",
    "mean_joels_models = macro_f1[macro_f1._name_or_path.str.contains('joelito')].mean_over_seeds.mean()\n",
    "mean_not_joels_models = macro_f1[macro_f1._name_or_path.str.contains('joelito')==False].mean_over_seeds.mean()\n",
    "print(mean_joels_models)\n",
    "print(mean_not_joels_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045393f",
   "metadata": {},
   "source": [
    "# 2. General information\n",
    "\n",
    "- When fetching the data from wandb, the data is processed and saved to a dataframe called `results` which is an object variable\n",
    "- **In contains only runs that are finished. Failed or crashed runs on wandb have been removed**\n",
    "- You can access the processed data like this `ra.results`\n",
    "- This data will be used to calculate the scores\n",
    "- All overviews with the calculated aggregate scores will be saved to dataframes as well, which can be accessed likewise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4ab53",
   "metadata": {},
   "source": [
    "# 2. Overviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7974afb",
   "metadata": {},
   "source": [
    "- The code was writing while some runs for certain had not been finished \n",
    "- Therefore, you can specify whether the generated overviews should consider only taks whose run had been completed entirely. This is automatically done by setting ```only_completed_tasks```to ```True``` as a default value.\n",
    "- In case you want to see interim results, including tasks that have not been completed yet, you can set ```only_completed_tasks```to ```False``` when intitiliazing:\n",
    "\n",
    "```\n",
    "ra = ResultAggregator(path_to_csv_export=\"current_wandb_results_unprocessed.csv\", only_completed_tasks=False)\n",
    "\n",
    "```\n",
    "\n",
    "- The code will create a bunch of different overviews that will be described in the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768888e7",
   "metadata": {},
   "source": [
    "## 2.1. Incomplete tasks\n",
    "\n",
    "- If you want to generate an overview of incomplete tasks you run the following: `ra.check_seed_per_task()`\n",
    "- This method will check if all task have the required number of finished runs with the seeds 1,2,3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_check = ra.check_seed_per_task()\n",
    "seed_check.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64d486",
   "metadata": {},
   "source": [
    "##  2.2. Overviews along scores\n",
    "\n",
    "- The method ```create_overview_of_results_per_seed()``` will generate an overview of the required score per seed and give a mean value. If not specified otherwise, the score defined during initializing will be used, i.e. `macro-f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75544011",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.create_overview_of_results_per_seed(only_completed_tasks=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ae001",
   "metadata": {},
   "source": [
    "- If you want to to generate an overview for another score, let's say `micro-f1`, you can pass the name of the score as a variable to the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0e7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra.create_overview_of_results_per_seed(score='micro-f1', only_completed_tasks=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e23f0",
   "metadata": {},
   "source": [
    "## 2.3 Report\n",
    "- As a shortcut, you can use the method `create_report()`\n",
    "- It will run `create_overview_of_results_per_seed` over different scores and give an overview of missing runs.\n",
    "- The overviews will be saved in an excel file called `report.xlsx` in the directory `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59beb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.create_report(only_completed_tasks=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf9dcb",
   "metadata": {},
   "source": [
    "- When running `create_report()` the generated overviews will be stored as dataframes in object variables that can be directly accessed.\n",
    "- Currently, the variables are:\n",
    "  - seed_check\n",
    "  - macro_f1_overview\n",
    "  - micro_f1_overview\n",
    "  - weighted_f1_overview\n",
    "  - accuracy_normalized_overview\n",
    "  \n",
    "- You can use them for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53514b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.macro_f1_overview[['finetuning_task', 'arithmetic_mean_over_seeds']].groupby('finetuning_task').mean().sort_values('arithmetic_mean_over_seeds').plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd758d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.accuracy_normalized_overview[['finetuning_task', 'arithmetic_mean_over_seeds']].groupby('finetuning_task').mean().sort_values('arithmetic_mean_over_seeds').plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddc2f2",
   "metadata": {},
   "source": [
    "### 2.4. Aggregated scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef01125",
   "metadata": {},
   "source": [
    "### 2.4.1. Language aggregate\n",
    "\n",
    "`DEFINITION`: *\"1. Average over configurations (e.g. judgment and unanimity) inside datasets, 2. Average over datasets, 3. Average over languages\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed72a40",
   "metadata": {},
   "source": [
    "- The aggregated score along languages (the language aggregate score) can be generated with the method `get_language_aggregated_score()`\n",
    "  - Since, the default score is `macro-f1`, the aggregate score is based on macro-f1. You can change this by defining another score when initializing the object (see description above).\n",
    "- The result dataframe is stored in the variable `language_aggregated_score`\n",
    "- When calling the method, the resulting dataframe is saved to a csv and xlsx file, called `language_aggregated_scores` in the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0030d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.get_language_aggregated_score(task_constraint=task_constraint,\n",
    "                     model_constraint=model_constraint)\n",
    "ra.language_aggregated_score.loc[multilingual_models]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b80f6",
   "metadata": {},
   "source": [
    "## - You can restrict the list of tasks you want to consider by passing a list of task names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef1c14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.get_language_aggregated_score(task_constraint=['german_argument_mining'])\n",
    "ra.language_aggregated_score.loc[multilingual_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6aace",
   "metadata": {},
   "source": [
    "### 2.4.2. Config aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55194b2",
   "metadata": {},
   "source": [
    "- The aggregated score along configs (the task config score) can be generated with the method `get_config_aggregated_score()`\n",
    "  - Since, the default score is `macro-f1`, the aggregate score is based on macro-f1. You can change this by defining another score when initializing the object (see description above).\n",
    "  - Per default the method calcualtes the average over config, dataset, language. To overwrite this behavious, i.e. to calculate the avergae score not over languages, you can set average_over_language=False. The resulting values should be similar to the aggregate scores we find on wandb\n",
    "- The result dataframe is stored in the variable `config_aggregated_score`\n",
    "- When calling the method, the resulting dataframe is saved to a csv and xlsx file\n",
    "  - Depending on whether you average over language or not the name of the file is called `config_aggregated_scores_simple` (not averaged over language) OR `config_aggregated_scores_average_over_language` (averaged over language) in the results folder\n",
    "  \n",
    "**The dataset aggregate score is built on top of the results of the config aggregate score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.get_config_aggregated_score(task_constraint=task_constraint)\n",
    "ra.config_aggregated_score.loc[multilingual_models]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8a758",
   "metadata": {},
   "source": [
    "### 2.4.3. Dataset aggregate\n",
    "`DEFINITION`: *\"1. Average over languages inside configurations, 2. Average over configurations (e.g. judgment and unanimity) inside datasets, 3. Average over Datasets\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e341ee",
   "metadata": {},
   "source": [
    "- The aggregated score along datasets (the dataset aggregate score) can be generated with the method `get_dataset_aggregated_score()`\n",
    "  - Since, the default score is `macro-f1`, the aggregate score is based on macro-f1. You can change this by defining another score when initializing the object (see description above).\n",
    "  - Per default the method calcualtes the average over config, dataset, language. To overwrite this behavious, i.e. to calculate the avergae score not over languages, you can set average_over_language=False. The resulting values should be similar to the aggregate scores we find on wandb\n",
    "- The result dataframe is stored in the variable `dataset_aggregated_score`\n",
    "- When calling the method, the resulting dataframe is saved to a csv and xlsx file\n",
    "  - Depending on whether you average over language or not the name of the file is called `dataset_aggregated_scores_simple` (not averaged over language) OR `dataset_aggregated_scores_average_over_language` (averaged over language) in the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e95f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra.get_dataset_aggregated_score(task_constraint=task_constraint, model_constraint=model_constraint)\n",
    "ra.dataset_aggregated_score.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95859a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.get_dataset_aggregated_score(task_constraint=task_constraint, model_constraint=model_constraint,average_over_language=False)\n",
    "ra.dataset_aggregated_score.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca66314",
   "metadata": {},
   "source": [
    "## 3. Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8702b",
   "metadata": {},
   "source": [
    "### 3.1. Task constraint\n",
    "- We want to calculate a task aggregate score\n",
    "- It might be useful, to remove some tasks from the calculations\n",
    "- This can be done by passing a list of tasks we want to consider; see the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results for all available tasks:')\n",
    "print('------------------------')\n",
    "ra.get_language_aggregated_score()\n",
    "ra.get_dataset_aggregated_score(average_over_language=True)\n",
    "\n",
    "lm = [\"distilbert-base-multilingual-cased\", \"microsoft/Multilingual-MiniLM-L12-H384\", \"xlm-roberta-base\", \"microsoft/mdeberta-v3-base\", \"xlm-roberta-large\"]\n",
    "\n",
    "for x in lm:\n",
    "    print(x)\n",
    "    print('\\t','language aggregated: ',ra.language_aggregated_score.at[x,'Agg.'])\n",
    "    print('\\t','dataset aggregated: ', ra.dataset_aggregated_score.at[x,'Agg.'])\n",
    "    print('-----------------------------------')\n",
    "\n",
    "print('\\n##########################\\n')\n",
    "\n",
    "tasks_we_are_interested_in = ['mapa_fine', 'german_argument_mining']\n",
    "print('Results for only selected tasks:', ', '.join(tasks_we_are_interested_in))\n",
    "print('------------------------')\n",
    "ra.get_language_aggregated_score(task_constraint=tasks_we_are_interested_in)\n",
    "ra.get_dataset_aggregated_score(average_over_language=True, task_constraint=tasks_we_are_interested_in)\n",
    "\n",
    "lm = [\"distilbert-base-multilingual-cased\", \"microsoft/Multilingual-MiniLM-L12-H384\", \"xlm-roberta-base\", \"microsoft/mdeberta-v3-base\", \"xlm-roberta-large\"]\n",
    "\n",
    "for x in lm:\n",
    "    print(x)\n",
    "    print('\\t','language aggregated: ',ra.language_aggregated_score.at[x,'Agg.'])\n",
    "    print('\\t','dataset aggregated: ', ra.dataset_aggregated_score.at[x,'Agg.'])\n",
    "    print('-----------------------------------')\n",
    "\n",
    "print('\\n##########################\\n')\n",
    "\n",
    "ra.reset_list_of_available_languages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0420481",
   "metadata": {},
   "source": [
    "### 3.2. Language constraint\n",
    "- We want to calculate a language aggregate score\n",
    "- It might be useful, to remove some languages from the calculations\n",
    "- This can be done with the method `remove_languages({LANGUAGE_ID})`\n",
    "- You can reset the initial number of languages with the method `reset_list_of_available_languages()`\n",
    "- This might be useful, if you want to gauge the influence of certain languages on the overall scores (see example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.reset_list_of_available_languages()\n",
    "\n",
    "print('Results with all available languages:')\n",
    "print('------------------------')\n",
    "ra.get_language_aggregated_score()\n",
    "ra.get_dataset_aggregated_score(average_over_language=True)\n",
    "\n",
    "lm = [\"distilbert-base-multilingual-cased\", \"microsoft/Multilingual-MiniLM-L12-H384\", \"xlm-roberta-base\", \"microsoft/mdeberta-v3-base\", \"xlm-roberta-large\"]\n",
    "\n",
    "for x in lm:\n",
    "    print(x)\n",
    "    print('\\t','language aggregated: ',ra.language_aggregated_score.at[x,'Agg.'])\n",
    "    print('\\t','dataset aggregated: ', ra.dataset_aggregated_score.at[x,'Agg.'])\n",
    "    print('-----------------------------------')\n",
    "\n",
    "print('\\n##########################\\n')\n",
    "\n",
    "ra.reset_list_of_available_languages()\n",
    "\n",
    "ra.remove_languages('pt')\n",
    "\n",
    "print('Results without Portuguese:')\n",
    "print('------------------------')\n",
    "ra.get_language_aggregated_score()\n",
    "ra.get_dataset_aggregated_score(average_over_language=True)\n",
    "\n",
    "lm = [\"distilbert-base-multilingual-cased\", \"microsoft/Multilingual-MiniLM-L12-H384\", \"xlm-roberta-base\", \"microsoft/mdeberta-v3-base\", \"xlm-roberta-large\"]\n",
    "\n",
    "for x in lm:\n",
    "    print(x)\n",
    "    print('\\t','language aggregated: ',ra.language_aggregated_score.at[x,'Agg.'])\n",
    "    print('\\t','dataset aggregated: ', ra.dataset_aggregated_score.at[x,'Agg.'])\n",
    "    print('-----------------------------------')\n",
    "\n",
    "print('\\n##########################\\n')\n",
    "\n",
    "ra.reset_list_of_available_languages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f09d05",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4d4e9",
   "metadata": {},
   "source": [
    "### 4.1. Prepare tables for LaTeX\n",
    "\n",
    "- In order for this to work, you need to make the following imports in your LaTeX document\n",
    "\n",
    "```\n",
    "\\usepackage{rotating}\n",
    "\\usepackage{adjustbox}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0da0c",
   "metadata": {},
   "source": [
    "### 4.2. Define functions and variables that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of mono- and multilingual models\n",
    "multilingual_models = [k for k,v in ra.meta_infos['model_language_lookup_table'].items() if v=='all' and k in model_constraint]\n",
    "monolingual_models = [k for k,v in ra.meta_infos['model_language_lookup_table'].items() if v!='all' and k in model_constraint]\n",
    "\n",
    "order_of_tasks_as_abbreviations = [\"BCD-J\", \"BCD-U\", \"GAM\", \"GLC-V\", \"GLC-C\", \"GLC-S\", \"SJP\", \"OTS-UL\", \"OTS-CT\", \"C19\", \"MEU-1\", \"MEU-2\", \"MEU-3\", \"GLN\", \"LNR\", \"LNB\", \"MAP-C\", \"MAP-F\"]\n",
    "order_of_datasets_as_abbreviations = [\"BCD\",\"GAM\",\"GLC\",\"SJP\", \"OTS\", \"C19\", \"MEU\", \"GLN\", \"LNR\", \"LNB\", \"MAP\"]\n",
    "order_of_multilingual_models = [\"microsoft/Multilingual-MiniLM-L12-H384\", \"distilbert-base-multilingual-cased\", \"microsoft/mdeberta-v3-base\", \"xlm-roberta-base\",\"xlm-roberta-large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remove \"nb\" for Lextreme\n",
    "\n",
    "ra.remove_languages(\"nb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9021ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We round all scores (macro-F1 values)\n",
    "\n",
    "def rename_multilingual_models(model_name):\n",
    "    if model_name=='distilbert-base-multilingual-cased':\n",
    "        return 'DistilBERT'\n",
    "    if model_name=='microsoft/Multilingual-MiniLM-L12-H384':\n",
    "        return 'MiniLM'\n",
    "    if model_name=='microsoft/mdeberta-v3-base':\n",
    "        return 'mDeBERTa v3'\n",
    "    if model_name=='xlm-roberta-base':\n",
    "        return 'XLM-R\\\\textsubscript{Base}'\n",
    "    if model_name=='xlm-roberta-large':\n",
    "        return 'XLM-R\\\\textsubscript{Large}'\n",
    "    if model_name=='google/mt5-small':\n",
    "        return 'mT5\\\\textsubscript{Small}'\n",
    "    else:\n",
    "        return model_name\n",
    "    \n",
    "def make_latex_bold(name):\n",
    "    \n",
    "    return \"\\textbf{\"+name +\"}\"\n",
    "\n",
    "def make_columns_latex_bold(df):\n",
    "    \n",
    "    df_copy = deepcopy(df)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        df_copy = df_copy.rename(columns={c:make_latex_bold(c)})\n",
    "    \n",
    "    return df_copy\n",
    "        \n",
    "        \n",
    "def round_scores(score):\n",
    "    if type(score)==float:\n",
    "        return float(round(score*100,1))\n",
    "    else:\n",
    "        return score\n",
    "    \n",
    "def insert_standard_deviation(overview_of_results_per_seed, _name_or_path, column_name, query):\n",
    "    \n",
    "    for r in overview_of_results_per_seed.to_dict(orient=\"records\"):\n",
    "        \n",
    "        if r[column_name]==query and r['_name_or_path']==_name_or_path:\n",
    "            return r['standard_deviation']\n",
    "        \n",
    "def get_mean_of_row_values(row_values):\n",
    "    #row_values = predict_macro_f1_average.loc['iarfmoose/roberta-base-bulgarian'].tolist()\n",
    "    \n",
    "    row_values = [x for x in row_values if type(x) in [float, int]]\n",
    "    \n",
    "    if len(row_values) > 0:\n",
    "        mean_value = mean(row_values)\n",
    "    \n",
    "        mean_value = round(mean_value,1)\n",
    "    else:\n",
    "        mean_value = 0\n",
    "        \n",
    "    return mean_value\n",
    "\n",
    "\n",
    "def insert_mean_values(df, column_name_for_mean_value):\n",
    "    \n",
    "    result_dict = dict()\n",
    "    for i, _ in df.iterrows():\n",
    "        row_values = df.loc[i].tolist()\n",
    "        row_values_mean = get_mean_of_row_values(row_values)\n",
    "        result_dict[i]=row_values_mean\n",
    "        \n",
    "    for i, _ in df.iterrows():\n",
    "        df.at[i,column_name_for_mean_value]=result_dict[i]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def try_literal_eval(value):\n",
    "    try:\n",
    "        return literal_eval(value)\n",
    "    except:\n",
    "        return value\n",
    "\n",
    "def replace_empty_string_with_zero(value):\n",
    "    \n",
    "    if value==\"\":\n",
    "        return np.NAN\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def highlight_max_values_in_colums(df):\n",
    "    \n",
    "    df_copy = deepcopy(df)\n",
    "    df_copy = df_copy.applymap(replace_empty_string_with_zero)\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    for col in columns:\n",
    "        \n",
    "        if len([v for v in df_copy[col] if type(try_literal_eval(v)) in [int, float]])>0:\n",
    "            try:\n",
    "                max_value = df_copy[col].max()\n",
    "                new_values = [v if v!=max_value else '\\\\bf '+str(v) for v in df_copy[col]]\n",
    "                df_copy[col]=new_values\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(df_copy[col])\n",
    "    \n",
    "    df_copy.fillna('', inplace=True)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def replace_empty_with_minus(value):\n",
    "    if value==\"\":\n",
    "        return \"-\"\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def create_latex_horizontal_table(df, caption=\"\", label=\"\", index=False, insert_minus=True):\n",
    "    \n",
    "    #Make column names bold\n",
    "    if insert_minus==True:\n",
    "        df = df.applymap(replace_empty_with_minus)\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    for c in columns:\n",
    "        c_new = \"\\textbf{\"+c+r\"}\"\n",
    "        df = df.rename(columns={c:c_new})\n",
    "    \n",
    "    pre_formatting = \"\"\n",
    "    pre_formatting = pre_formatting + \"\\\\begin{table*}[t]\\n\"\n",
    "    pre_formatting = pre_formatting + \"\\\\centering\\n\"\n",
    "    pre_formatting = pre_formatting + \"\\\\resizebox{\\\\textwidth}{!}{\\n\"\n",
    "    \n",
    "    after_formatting = \"\"\n",
    "    after_formatting = after_formatting + \"}\\n\"\n",
    "    after_formatting = after_formatting + \"\\caption{\"+caption+\"}\\n\"\n",
    "    after_formatting = after_formatting + \"\\label{\"+label+\"}\\n\"\n",
    "    after_formatting = after_formatting + \"\\end{table*}\"\n",
    "    \n",
    "    with pd.option_context(\"max_colwidth\", 1000):\n",
    "        df_latex = df.to_latex(index=index, escape=False)\n",
    "    \n",
    "    return pre_formatting + df_latex + after_formatting\n",
    "    \n",
    "\n",
    "def check_model_type(model_name, only_type=False):\n",
    "    \n",
    "    '''Checks if model is general or legal.'''\n",
    "    for model_type, info in ra.meta_infos['language_models'].items():\n",
    "        for language, size_infos in info.items():\n",
    "            for size, model_list in size_infos.items():\n",
    "                for model in model_list:\n",
    "                    if model == model_name:\n",
    "                        if only_type:\n",
    "                            return model_type\n",
    "                        else:\n",
    "                            if model in multilingual_models:\n",
    "                                return \"multiling. \" + model_type\n",
    "                            else:\n",
    "                                return \"monoling. \" + model_type\n",
    "\n",
    "    \n",
    "    \n",
    "def create_latex_sideways_table(df, caption=\"\", label=\"\", index=True, insert_minus=True):\n",
    "    \n",
    "    \n",
    "    #Make column names bold\n",
    "    \n",
    "    if insert_minus==True:\n",
    "        df = df.applymap(replace_empty_with_minus)\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    for c in columns:\n",
    "        df = df.rename(columns={c : \"\\textbf{\"+c+r\"}\"})\n",
    "        \n",
    "    pre_formatting = \"\"\n",
    "    pre_formatting = pre_formatting + \"\\\\begin{sidewaystable}\\n\"\n",
    "    pre_formatting = pre_formatting + \"\\\\begin{adjustbox}{width=1\\\\textwidth}\\n\"\n",
    "    pre_formatting = pre_formatting + \"\\medium\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    after_formatting = \"\"\n",
    "    after_formatting = after_formatting + \"\\end{adjustbox}\\n\"\n",
    "    after_formatting = after_formatting + \"\\caption{\"+caption+\"}\\n\"\n",
    "    after_formatting = after_formatting + \"\\label{\"+label+\"}\\n\"\n",
    "    after_formatting = after_formatting + \"\\end{sidewaystable}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    with pd.option_context(\"max_colwidth\", 1000):\n",
    "        df_latex = df.to_latex(index=index, escape=False)\n",
    "    \n",
    "    return pre_formatting + df_latex + after_formatting\n",
    "    \n",
    "def combine_latex_sideways_tables(table_1, table_2, distance=\"1cm\"):\n",
    "    table_1 = '\\n'.join(table_1.split('\\n')[1:-2])\n",
    "    table_2 = '\\n'.join(table_2.split('\\n')[1:-2])\n",
    "    \n",
    "    pre_formatting = \"\\\\begin{sidewaystable}\\n\"\n",
    "    after_formatting = \"\\\\end{sidewaystable}\\n\"\n",
    "    \n",
    "    return pre_formatting + table_1 +\"\\\\vspace{\"+distance+\"}\"+table_2 + after_formatting\n",
    "    \n",
    "\n",
    "def create_average_aggregates(target, score, language_types=None, highlight_max_values=True):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Generates scores based on normal average for predcit and eval values.\n",
    "    Gets average over all seeds per task.\n",
    "    Gets average over targets.\n",
    "    The target is either the dataset or the language.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target : The value you want to create the aggregate score with. It canbe either 'language' or 'dataset'.\n",
    "    score : Define which score you want to use. If you choose 'macro-f1', it will be converted to 'predict/_macro_f1'. If you want to use the values from the eval dataset, use the prefix 'eval_', i.e. 'eval/macro-f1'. \n",
    "    '''\n",
    "    \n",
    "    macro_f1_first = ra.create_overview_of_results_per_seed(score=score,only_completed_tasks=False, task_constraint=task_constraint, model_constraint=model_constraint)\n",
    "    macro_f1 = macro_f1_first[[\"_name_or_path\", target, \"arithmetic_mean_over_seeds\"]]\n",
    "    macro_f1 = macro_f1.sort_values(target)\n",
    "    df_aggregated = pd.pivot_table(macro_f1,index=\"_name_or_path\", columns=target, values=\"arithmetic_mean_over_seeds\", aggfunc=np.mean)\n",
    "    \n",
    "    if language_types == 'multi':\n",
    "        df_aggregated = df_aggregated.loc[multilingual_models]\n",
    "    elif language_types == 'mono':\n",
    "        df_aggregated = df_aggregated.loc[monolingual_models]\n",
    "            \n",
    "\n",
    "    \n",
    "    df_aggregated.columns.name=\"\"\n",
    "    df_aggregated.index.rename(\"Model\", inplace=True)\n",
    "    df_aggregated.dropna(axis=1, how='all', inplace=True)\n",
    "    df_aggregated.fillna('', inplace=True)\n",
    "    \n",
    "    if target==\"language\":\n",
    "        df_aggregated = df_aggregated.rename(columns={'all':'multilingual'})\n",
    "    \n",
    "    df_aggregated = df_aggregated.applymap(round_scores)\n",
    "    df_aggregated.fillna(\"\", inplace=True)\n",
    "    first_column = 'Mean' #over '+re.sub('_',' ',target)+'s'\n",
    "    #df_aggregated[first_column] = df_aggregated.mean(axis=1, numeric_only=True) # Does not work\n",
    "    \n",
    "    # Insert mean values\n",
    "    df_aggregated = insert_mean_values(df_aggregated, first_column)\n",
    "        \n",
    "\n",
    "\n",
    "    columns = df_aggregated.columns.tolist()\n",
    "    df_aggregated = df_aggregated.applymap(lambda x: str(x))\n",
    "    df_aggregated = df_aggregated[[first_column] + [col for col in columns if col not in first_column]]\n",
    "    \n",
    "    if highlight_max_values:\n",
    "        df_aggregated = highlight_max_values_in_colums(df_aggregated)\n",
    "    \n",
    "    # Insert standard deviation\n",
    "    for t in macro_f1_first[target].unique():\n",
    "        for _name_or_path in df_aggregated.index.tolist():\n",
    "            standard_deviation = insert_standard_deviation(macro_f1_first, _name_or_path, target, t)\n",
    "            if df_aggregated.at[_name_or_path, t]:\n",
    "                df_aggregated.at[_name_or_path, t] = df_aggregated.at[_name_or_path, t] + ' (±'+str(round_scores(standard_deviation))+') '  \n",
    "    \n",
    "    \n",
    "    # Insert abbreviations\n",
    "    if target == \"finetuning_task\":\n",
    "        for finetuning_task, abbreviation in ra.meta_infos[\"task_abbreviations\"].items():\n",
    "            df_aggregated.rename(columns={finetuning_task: abbreviation}, inplace=True)\n",
    "            \n",
    "    rest_columns = [c for c in df_aggregated.columns if c not in order_of_tasks_as_abbreviations]\n",
    "    df_aggregated = df_aggregated[rest_columns + order_of_tasks_as_abbreviations]\n",
    "    \n",
    "    '''for x in ['MEU-2', 'MEU-3']:\n",
    "        if x in df_aggregated.columns.tolist():\n",
    "            del df_aggregated[x]'''\n",
    "    \n",
    "    df_aggregated = make_columns_latex_bold(df_aggregated)\n",
    "    if language_types==\"multi\":\n",
    "        df_aggregated = df_aggregated.reindex(index = order_of_multilingual_models)\n",
    "    df_aggregated.index = df_aggregated.index.map(rename_multilingual_models)        \n",
    "    \n",
    "    return df_aggregated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f152f",
   "metadata": {},
   "source": [
    "### 4.3.  Compare eval and prediction mean of macro-F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f57a40",
   "metadata": {},
   "source": [
    "### 4.3.1. Create Averages over finetuning tasks\n",
    "#### 4.3.1.1. Multilingual models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e2fe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "language_types = 'multi'\n",
    "# predict macro-f1\n",
    "predict_macro_f1_average = create_average_aggregates(target='finetuning_task',score='macro-f1', language_types=language_types)\n",
    "\n",
    "# eval macro-f1\n",
    "eval_macro_f1_average = create_average_aggregates(target='finetuning_task',score='eval/macro-f1', language_types=language_types)\n",
    "\n",
    "combined_macro_f1_average = deepcopy(eval_macro_f1_average)\n",
    "\n",
    "\n",
    "# This is not needed anymore\n",
    "'''for i, _ in combined_macro_f1_average.iterrows():\n",
    "    for c in combined_macro_f1_average.columns.tolist():\n",
    "        eval_score = eval_macro_f1_average.at[i, c]\n",
    "        predict_score = predict_macro_f1_average.at[i, c]\n",
    "        combined_macro_f1_average.at[i, c]=str(eval_score)+' / '+str(predict_score)'''\n",
    "        \n",
    "\n",
    "predict_macro_f1_average_latext = create_latex_sideways_table(predict_macro_f1_average, caption=\"Arithmetic mean of macro-F1 and the standard deviation over all seeds for multilingual models from the test set. The best scores are in bold.\")\n",
    "eval_macro_f1_average_latex = create_latex_sideways_table(eval_macro_f1_average, caption=\"Arithmetic mean of macro-F1 and the standard deviation over all seeds for multilingual models from the validation set. The best scores are in bold.\")\n",
    "x = combine_latex_sideways_tables(eval_macro_f1_average_latex, predict_macro_f1_average_latext)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a755f65",
   "metadata": {},
   "source": [
    "#### 4.3.1.2. Monolingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e9995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc409c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "language_types = 'mono'\n",
    "\n",
    "# eval macro-f1\n",
    "eval_macro_f1_average = create_average_aggregates(target='finetuning_task',score='eval/macro-f1', language_types=language_types, highlight_max_values=False)\n",
    "\n",
    "# predict macro-f1\n",
    "predict_macro_f1_average = create_average_aggregates(target='finetuning_task',score='macro-f1', language_types=language_types, highlight_max_values=False)\n",
    "\n",
    "combined_macro_f1_average = deepcopy(eval_macro_f1_average)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "eval_macro_f1_average_latex = create_latex_sideways_table(eval_macro_f1_average, caption=\"Arithmetic mean of macro-F1 and the standard deviation over all seeds for monolingual models from the validation set.\")\n",
    "predict_macro_f1_average_latext = create_latex_sideways_table(predict_macro_f1_average, caption=\"Arithmetic mean of macro-F1 and the standard deviation over all seeds for monolingual models from the test set.\")\n",
    "x = combine_latex_sideways_tables(eval_macro_f1_average_latex, predict_macro_f1_average_latext)\n",
    "\n",
    "print(eval_macro_f1_average_latex)\n",
    "print(predict_macro_f1_average_latext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239648e",
   "metadata": {},
   "source": [
    "### 4.4. Create language and dataset aggregate scores \n",
    "#### 4.4.1. First only multilingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ead4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_highes_score(language, which):\n",
    "    ra.get_language_aggregated_score()\n",
    "    las = ra.language_aggregated_score\n",
    "    las = las.applymap(round_scores)\n",
    "    las = las.reset_index()\n",
    "    las['Type'] = las.Model.apply(lambda x: check_model_type(x))\n",
    "    las = las.set_index('Model')\n",
    "    las_mono_general = las[(las.Type==\"monoling. general\") ]\n",
    "    las_mono_legal = las[las.Type==\"monoling. legal\"]\n",
    "\n",
    "    if which==\"general\":\n",
    "        df_copy = deepcopy(las_mono_general)\n",
    "    elif which==\"legal\":\n",
    "        df_copy = deepcopy(las_mono_legal)\n",
    "    df_copy = df_copy.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    res = df_copy[language].max()\n",
    "    return res\n",
    "\n",
    "\n",
    "ra.get_language_aggregated_score(task_constraint=task_constraint, model_constraint=model_constraint)\n",
    "language_aggregated_score = ra.language_aggregated_score\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score.loc[multilingual_models]\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.applymap(round_scores)\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.reindex(index = order_of_multilingual_models)\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.reset_index()\n",
    "empty_row = pd.DataFrame([{c:'' for c in language_aggregated_score_multilingual_models.columns}])\n",
    "language_aggregated_score_multilingual_models = pd.concat([language_aggregated_score_multilingual_models, empty_row])\n",
    "language_aggregated_score_multilingual_models = pd.concat([language_aggregated_score_multilingual_models, empty_row])\n",
    "language_aggregated_score_multilingual_models.reset_index(inplace=True)\n",
    "index_for_best_legal_model = language_aggregated_score_multilingual_models.index.tolist()[-2]\n",
    "index_for_best_general_model = language_aggregated_score_multilingual_models.index.tolist()[-1]\n",
    "language_aggregated_score_multilingual_models.at[index_for_best_legal_model, 'Model']='NativeLegalBERT'\n",
    "language_aggregated_score_multilingual_models.at[index_for_best_general_model, 'Model']='NativeBERT'\n",
    "for c in language_aggregated_score_multilingual_models.columns:\n",
    "    if len(c)==2:\n",
    "        for i,_ in language_aggregated_score_multilingual_models.iterrows():\n",
    "            print(i, c)\n",
    "            best_legal_model = find_highes_score(c, 'legal')\n",
    "            best_general_model = find_highes_score(c, 'general')\n",
    "            print(best_legal_model)\n",
    "            print(best_general_model)\n",
    "            language_aggregated_score_multilingual_models.at[index_for_best_legal_model, c]=best_legal_model\n",
    "            language_aggregated_score_multilingual_models.at[index_for_best_general_model, c]=best_general_model\n",
    "            del best_general_model\n",
    "            del best_legal_model\n",
    "            \n",
    "del language_aggregated_score_multilingual_models['index']\n",
    "#language_aggregated_score_multilingual_models_latex = create_latex_horizontal_table(language_aggregated_score_multilingual_models, caption=\"Language aggregate scores for multilingual models. The best scores are in bold.\", label=\"language_aggregate_multilingual_models\")\n",
    "#print(language_aggregated_score_multilingual_models_latex) #Does not work\n",
    "col_name_agg = \"Agg.\"\n",
    "columns_las = [c for c in language_aggregated_score_multilingual_models.columns.tolist() if c != col_name_agg]\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models[columns_las+[col_name_agg]]\n",
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.rename(columns={col_name_agg:\"Agg.\"})\n",
    "\n",
    "\n",
    "ra.get_dataset_aggregated_score(task_constraint=task_constraint, model_constraint=model_constraint)\n",
    "dataset_aggregated_score = insert_abbreviations_in_columns(ra.dataset_aggregated_score)\n",
    "rest_columns = [c for c in dataset_aggregated_score.columns if c not in order_of_datasets_as_abbreviations]\n",
    "dataset_aggregated_score = dataset_aggregated_score[order_of_datasets_as_abbreviations+rest_columns]\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score.loc[multilingual_models]\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models.applymap(round_scores)\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models.reindex(index = order_of_multilingual_models)\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models.reset_index()\n",
    "#dataset_aggregated_score_multilingual_models_latex = create_latex_horizontal_table(dataset_aggregated_score_multilingual_models, caption=\"Dataset aggregate scores for multilingual models\", label=\"dataset_aggregate_multilingual_models\")\n",
    "#print(dataset_aggregated_score_multilingual_models_latex) #Does not work\n",
    "col_name_agg = \"Agg.\"\n",
    "columns_las = [c for c in dataset_aggregated_score_multilingual_models.columns.tolist() if c != col_name_agg]\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models[columns_las+[col_name_agg]]\n",
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models.rename(columns={col_name_agg:\"Agg.\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b68018",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_aggregated_score_multilingual_models = dataset_aggregated_score_multilingual_models.applymap(rename_multilingual_models)\n",
    "dataset_aggregated_score_multilingual_models = highlight_max_values_in_colums(dataset_aggregated_score_multilingual_models)\n",
    "dataset_aggregated_score_multilingual_models = make_columns_latex_bold(dataset_aggregated_score_multilingual_models)\n",
    "print(dataset_aggregated_score_multilingual_models.to_latex(index=False, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdb3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.applymap(rename_multilingual_models)\n",
    "language_aggregated_score_multilingual_models = highlight_max_values_in_colums(language_aggregated_score_multilingual_models)\n",
    "language_aggregated_score_multilingual_models = make_columns_latex_bold(language_aggregated_score_multilingual_models)\n",
    "print(language_aggregated_score_multilingual_models.to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec03435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "language_aggregated_score_multilingual_models = language_aggregated_score_multilingual_models.applymap(rename_multilingual_models)\n",
    "language_aggregated_score_multilingual_models = highlight_max_values_in_colums(language_aggregated_score_multilingual_models)\n",
    "language_aggregated_score_multilingual_models = make_columns_latex_bold(language_aggregated_score_multilingual_models)\n",
    "print(language_aggregated_score_multilingual_models.to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf88904",
   "metadata": {},
   "source": [
    "- At the end, we want to add rows with the best results for the native generic and legal models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacb96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(language_aggregated_score_multilingual_models.applymap(replace_empty_with_minus).to_latex(index=False, escape=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_aggregated_score_multilingual_models.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f440c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8248a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a90f53e",
   "metadata": {},
   "source": [
    "### 4.4. Then monolingual models\n",
    "\n",
    "- For monolingual models, only the aggregate score suffices, because it is the same as the score for their specific language\n",
    "- Therfore, we get rid of the columns, except of the column `aggregate score`\n",
    "- But, for each model name we add in brackets information about their specific language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919c715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra.get_language_aggregated_score()\n",
    "language_aggregated_score = ra.language_aggregated_score\n",
    "ra.get_dataset_aggregated_score()\n",
    "dataset_aggregated_score = ra.dataset_aggregated_score\n",
    "language_aggregated_score = language_aggregated_score.loc[monolingual_models]\n",
    "language_aggregated_score = language_aggregated_score[['aggregated_score']]\n",
    "language_aggregated_score['language'] = language_aggregated_score.index.map(find_language)\n",
    "language_aggregated_score = language_aggregated_score.applymap(round_scores)\n",
    "language_aggregated_score = language_aggregated_score[['language', 'aggregated_score']]\n",
    "language_aggregated_score.rename(columns={'aggregated_score':'language aggregated score'}, inplace=True)\n",
    "\n",
    "\n",
    "dataset_aggregated_score = ra.dataset_aggregated_score\n",
    "dataset_aggregated_score = dataset_aggregated_score.loc[monolingual_models]\n",
    "dataset_aggregated_score = dataset_aggregated_score.applymap(round_scores)\n",
    "dataset_aggregated_score.rename(columns={'aggregated_score':'dataset aggregated score'}, inplace=True)\n",
    "\n",
    "combined_scores = language_aggregated_score.join(dataset_aggregated_score, how='outer')\n",
    "del combined_scores['dataset aggregated score']\n",
    "combined_scores.rename(columns={'language aggregated score': 'Agg.', 'language':'Language'}, inplace=True)\n",
    "combined_scores = make_columns_latex_bold(combined_scores)\n",
    "combined_scores = highlight_max_values_in_colums(combined_scores)\n",
    "\n",
    "pre_formatting = \"\"\n",
    "pre_formatting = pre_formatting + \"\\\\begin{sidewaystable}\\n\"\n",
    "#pre_formatting = pre_formatting + \"\\centering\\n\"\n",
    "pre_formatting = pre_formatting + \"\\\\begin{adjustbox}{width=1\\\\textwidth}\\n\"\n",
    "pre_formatting = pre_formatting + \"\\medium\\n\"\n",
    "\n",
    "after_formatting = \"\"\n",
    "after_formatting = after_formatting + \"\\end{adjustbox}\\n\"\n",
    "after_formatting = after_formatting + \"\\caption{Dataset and Language aggregate scores for monolingual models. The best scores are in bold.}\\n\"\n",
    "after_formatting = after_formatting + \"\\label{dataset_language_aggregate_for_monolingual_models}\\n\"\n",
    "after_formatting = after_formatting + \"\\end{sidewaystable}\\n\"\n",
    "#pre_formatting = pre_formatting + \"\\n\"\n",
    "\n",
    "dataset_aggregated_score_for_latex = pre_formatting + combined_scores.to_latex(escape=False) + after_formatting\n",
    "print(dataset_aggregated_score_for_latex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f7c6b",
   "metadata": {},
   "source": [
    "- That was not sufficient. We want to compare monolingual models directly with multilingual models\n",
    "- Therefore, for each language we will create table which will contain\n",
    "  - The datasets that contain that specific language\n",
    "  - The language aggregate score from all multilingual model based on the pre-selected datasets\n",
    "  - The language aggregate score from the monolingual models relevant for the pre-selected language based on the pre-selected datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff43b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multilingual_models = [model for model, lang in ra.meta_infos['model_language_lookup_table'].items() if lang=='all']\n",
    "\n",
    "language_dict = {\n",
    "        \"bg\": \"Bulgarian\",\n",
    "        \"cs\": \"Czech\",\n",
    "        \"da\": \"Danish\",\n",
    "        \"de\": \"German\",\n",
    "        \"el\": \"Greek\",\n",
    "        \"en\": \"English\",\n",
    "        \"es\": \"Spanish\",\n",
    "        \"et\": \"Estonian\",\n",
    "        \"fi\": \"Finnish\",\n",
    "        \"fr\": \"French\",\n",
    "        \"ga\": \"Irish\",\n",
    "        \"hu\": \"Hungarian\",\n",
    "        \"it\": \"Italian\",\n",
    "        \"nl\": \"Dutch\",\n",
    "        \"pl\": \"Polish\",\n",
    "        \"pt\": \"Portuguese\",\n",
    "        \"ro\": \"Romanian\",\n",
    "        \"sk\": \"Slovak\",\n",
    "        \"sv\": \"Swedish\"\n",
    "    }\n",
    "\n",
    "relevant_languages = set()\n",
    "\n",
    "\n",
    "for model, language in ra.meta_infos['model_language_lookup_table'].items():\n",
    "    if language != 'all':\n",
    "        relevant_languages.add(language)\n",
    "        \n",
    "relevant_languages = sorted(list(relevant_languages))\n",
    "\n",
    "def get_relevant_monoling_models_for_language(lang):\n",
    "    relevant_models = list()\n",
    "    for model, language in ra.meta_infos['model_language_lookup_table'].items():\n",
    "        if language==lang:\n",
    "            relevant_models.append(model)\n",
    "    return relevant_models\n",
    "\n",
    "def get_relevant_tasks_for_language(lang):\n",
    "    relevant_tasks = list()\n",
    "    for task, languages in ra.meta_infos['task_language_mapping'].items():\n",
    "        for language in languages:\n",
    "            if language==lang:\n",
    "                relevant_tasks.append(task)\n",
    "    return relevant_tasks\n",
    "\n",
    "\n",
    "sample_model = 'iarfmoose/roberta-base-bulgarian'\n",
    "\n",
    "                            \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "#for lang in relevant_languages:\n",
    "def get_dataset_agg_for_lang(lang):\n",
    "    relevant_models = get_relevant_monoling_models_for_language(lang)\n",
    "    relevant_tasks = get_relevant_tasks_for_language(lang)\n",
    "    ra.get_dataset_aggregated_score(task_constraint=relevant_tasks)\n",
    "    dataset_aggregated_score = ra.dataset_aggregated_score\n",
    "    dataset_aggregated_score = dataset_aggregated_score.loc[relevant_models + multilingual_models]\n",
    "    dataset_aggregated_score = dataset_aggregated_score.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    dataset_aggregated_score = dataset_aggregated_score.applymap(round_scores)\n",
    "    dataset_aggregated_score.reset_index(inplace=True)\n",
    "    dataset_aggregated_score['Type']=dataset_aggregated_score.Model.apply(check_model_type)\n",
    "    dataset_aggregated_score.rename(columns={'aggregated_score':'Agg.'}, inplace=True)\n",
    "    datasets_abb = set([ra.meta_infos[\"config_to_dataset\"][t] for t in relevant_tasks])\n",
    "    datasets_abb = [ra.meta_infos['dataset_abbreviations'][t] for t in datasets_abb]\n",
    "    datasets_abb = [a for a in order_of_datasets_as_abbreviations if a in datasets_abb]\n",
    "    dataset_aggregated_score = dataset_aggregated_score[['Model', 'Type']+datasets_abb+['Agg.']]\n",
    "    dataset_aggregated_score = dataset_aggregated_score.dropna(axis=1, how='all')\n",
    "    remaining_monolingual = [m for m in dataset_aggregated_score.Model.tolist() if m not in order_of_multilingual_models]\n",
    "    dataset_aggregated_score = dataset_aggregated_score.set_index('Model')\n",
    "    dataset_aggregated_score = dataset_aggregated_score.loc[order_of_multilingual_models+remaining_monolingual]\n",
    "    dataset_aggregated_score = dataset_aggregated_score.reset_index()\n",
    "    dataset_aggregated_score['Model'] = dataset_aggregated_score.Model.apply(rename_multilingual_models)\n",
    "    #dataset_aggregated_score = dataset_aggregated_score.set_index('Model')\n",
    "    dataset_aggregated_score = highlight_max_values_in_colums(dataset_aggregated_score)\n",
    "    return dataset_aggregated_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31dc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for lang in relevant_languages:\n",
    "    caption = \"Dataset aggregate for all multilingual models and models pretrained on \"+language_dict[lang]+\" and datasets containing samples in \"+language_dict[lang]+\". The best scores are in bold.\"\n",
    "    label = \"dataset_aggregate_for_lang_\"+lang\n",
    "    latext_table = create_latex_horizontal_table(get_dataset_agg_for_lang(lang), index=False, caption=caption, label=label)\n",
    "    print(latext_table)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9f5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c5a5afe",
   "metadata": {},
   "source": [
    "## 5. Overview of splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare everything\n",
    "\n",
    "def get_list_of_languages(dataset_name, df):\n",
    "    list_of_languages = sorted(list(df[df.Dataset==dataset_name].Language.unique()))\n",
    "    list_of_languages = [l for l in list_of_languages if l!=\"all\"]\n",
    "    return ', '.join(list_of_languages)\n",
    "\n",
    "monolingual_tasks = set()\n",
    "multilingual_tasks = set()\n",
    "\n",
    "for task, list_of_langs in ra.meta_infos['task_language_mapping'].items():\n",
    "    if len(list_of_langs)>1:\n",
    "        multilingual_tasks.add(task)\n",
    "    else:\n",
    "        monolingual_tasks.add(task)\n",
    "        \n",
    "\n",
    "def check_if_basic_configuration(finetuning_task, language):\n",
    "    \n",
    "    if finetuning_task in multilingual_tasks and language=='all':\n",
    "        return True\n",
    "    elif finetuning_task in monolingual_tasks:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "sizes = pd.read_excel(\"Results from the Literature.xlsx\", sheet_name=\"Sizes\")\n",
    "\n",
    "sizes['for_basic_overview'] = sizes.apply(lambda x: check_if_basic_configuration(x['finetuning_task'], x['language']), axis=1)\n",
    "\n",
    "\n",
    "sizes['number_of_examples'] = sizes['number_of_examples'].apply(int)\n",
    "sizes['number_of_labels'] = sizes['number_of_labels'].apply(int)\n",
    "\n",
    "# Add dataset for each finetuning task\n",
    "sizes['Dataset'] = sizes.finetuning_task.apply(lambda x: ra.meta_infos[\"config_to_dataset\"][x])\n",
    "sizes[\"Jurisdiction\"]=sizes.Dataset.apply(lambda x: \", \".join(ra.meta_infos[\"dataset_jurisdiction\"][x]))\n",
    "\n",
    "sizes['finetuning_task'] = sizes['finetuning_task'].apply(lambda x: ra.meta_infos['task_abbreviations'][x])\n",
    "\n",
    "sizes['Dataset'] = sizes['Dataset'].apply(lambda x: ra.meta_infos['dataset_abbreviations'][x])\n",
    "\n",
    "sizes = sizes.rename(columns={\"finetuning_task\": \"Task\", \"split\":\"Split\",\"number_of_examples\": \"Num. of ex.\",\"number_of_labels\": \"Num. of labels\", \"language\": \"Language\"})\n",
    "sizes = sizes[[\"Dataset\", \"Task\", \"Split\", \"Language\", \"Num. of ex.\",\"Num. of labels\", \"for_basic_overview\", \"Jurisdiction\"]]\n",
    "sizes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a2f3f",
   "metadata": {},
   "source": [
    "### 5.1. Create table for the paper with the basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7c245",
   "metadata": {},
   "source": [
    "- Now, we will create an overview or the number of samples per split, number of labels per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bff49d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sizes(df, finetuning_task, split, target):\n",
    "    \n",
    "    return df[(df[\"Split\"]==split) & (df[\"Task\"]==finetuning_task)][target].tolist()[0]\n",
    "\n",
    "def insert_split_specific_num_ex(df, task, target):\n",
    "    \n",
    "    num_ex_train = get_sizes(df=df, split=\"train\", target=target, finetuning_task=task)\n",
    "    num_ex_eval = get_sizes(df=df, split=\"validation\", target=target, finetuning_task=task)\n",
    "    num_ex_test = get_sizes(df=df, split=\"test\", target=target, finetuning_task=task)\n",
    "    \n",
    "    result = [str(num_ex_train),str(num_ex_eval),str(num_ex_test)]\n",
    "    return \" / \".join(result)  \n",
    "\n",
    "\n",
    "\n",
    "subset_for_basic_overview = sizes[sizes[\"for_basic_overview\"]==True]\n",
    "look_up = deepcopy(subset_for_basic_overview)\n",
    "\n",
    "subset_for_basic_overview[\"Languages\"] = subset_for_basic_overview.Dataset.apply(lambda x: get_list_of_languages(x, sizes))\n",
    "subset_for_basic_overview[\"Num. of labels\"] = subset_for_basic_overview.Task.apply(lambda x: insert_split_specific_num_ex(look_up, x, \"Num. of labels\"))\n",
    "subset_for_basic_overview[\"Num. of ex.\"] = subset_for_basic_overview.Task.apply(lambda x: insert_split_specific_num_ex(look_up, x, \"Num. of ex.\"))\n",
    "subset_for_basic_overview = subset_for_basic_overview[[\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\",\"Jurisdiction\",\"Languages\"]]\n",
    "subset_for_basic_overview = subset_for_basic_overview.drop_duplicates([\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\",\"Jurisdiction\",\"Languages\"])\n",
    "subset_for_basic_overview = subset_for_basic_overview.set_index('Task')\n",
    "subset_for_basic_overview = subset_for_basic_overview.reindex(index = order_of_tasks_as_abbreviations)\n",
    "subset_for_basic_overview = subset_for_basic_overview.reset_index()\n",
    "subset_for_basic_overview = subset_for_basic_overview[[\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\",\"Jurisdiction\",\"Languages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fe168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples and number of labels\n",
    "cols = [\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\"]\n",
    "print(create_latex_horizontal_table(subset_for_basic_overview[cols].sort_values(cols), caption=\"Overview of datasets and their tasks. The fields \\\\textit{Num. of ex.} (Number of examples) and \\\\textit{Num. of labels} (Number of labels) provide the values for the splits train, validation, test. For a detailed overview of for the language-specific subsets of each multilingual task see Table XY.\", label=\"datasets_general_overview_num_of_ex_and_labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd324db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Jurisdiction and Languages\n",
    "\n",
    "print(create_latex_horizontal_table(subset_for_basic_overview[[\"Dataset\",\"Jurisdiction\",\"Languages\"]].drop_duplicates(), caption=\"Overview of datasets and the jurisdiction as well as the languages that they cover.\", label=\"datasets_general_overview_juris_languages\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2677653",
   "metadata": {},
   "source": [
    "### 5.2. Create tables for language-specific subsets of multilingual tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad44d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_for_language_specific_overview = sizes[sizes[\"for_basic_overview\"]==False]\n",
    "look_up = deepcopy(subset_for_language_specific_overview)\n",
    "\n",
    "def get_sizes(df, finetuning_task, split, target, lang):\n",
    "    \n",
    "    return df[(df[\"Split\"]==split) & (df[\"Task\"]==finetuning_task) & (df[\"Language\"]==lang)][target].tolist()[0]\n",
    "\n",
    "def insert_split_specific_num_ex(df, task, target, lang):\n",
    "    \n",
    "    num_ex_train = get_sizes(df=df, split=\"train\", target=target, finetuning_task=task, lang=lang)\n",
    "    num_ex_eval = get_sizes(df=df, split=\"validation\", target=target, finetuning_task=task, lang=lang)\n",
    "    num_ex_test = get_sizes(df=df, split=\"test\", target=target, finetuning_task=task, lang=lang)\n",
    "    \n",
    "    result = [str(num_ex_train),str(num_ex_eval),str(num_ex_test)]\n",
    "    return \" / \".join(result) \n",
    "\n",
    "\n",
    "subset_for_language_specific_overview[\"Languages\"] = subset_for_language_specific_overview.Dataset.apply(lambda x: get_list_of_languages(x, sizes))\n",
    "subset_for_language_specific_overview[\"Num. of labels\"] = subset_for_language_specific_overview.apply(lambda x: insert_split_specific_num_ex(look_up, x[\"Task\"], \"Num. of labels\",lang=x[\"Language\"]), axis=1)\n",
    "subset_for_language_specific_overview[\"Num. of ex.\"] = subset_for_language_specific_overview.apply(lambda x: insert_split_specific_num_ex(look_up, x[\"Task\"], \"Num. of ex.\",lang=x[\"Language\"]), axis=1)\n",
    "subset_for_language_specific_overview = subset_for_language_specific_overview[[\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\",\"Language\"]]\n",
    "subset_for_language_specific_overview = subset_for_language_specific_overview.drop_duplicates([\"Dataset\",\"Task\",\"Num. of ex.\",\"Num. of labels\",\"Language\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4bb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_for_language_specific_overview_num_ex = subset_for_language_specific_overview.pivot(index=[\"Language\"], columns=\"Task\", values=\"Num. of ex.\").fillna(\"\").reset_index()\n",
    "rest_columns = [c for c in subset_for_language_specific_overview_num_ex.columns if c not in order_of_tasks_as_abbreviations]\n",
    "subset_for_language_specific_overview_num_ex= subset_for_language_specific_overview_num_ex[rest_columns + [a for a in order_of_tasks_as_abbreviations if a in subset_for_language_specific_overview_num_ex.columns.tolist()]]\n",
    "caption = \"Overview of the number of examples for each language-specific subset of multilingual tasks. The order of the values is train / validation / test.\"\n",
    "label = \"subset_for_language_specific_overview_num_ex\"\n",
    "print(create_latex_sideways_table(subset_for_language_specific_overview_num_ex, index=False, caption=caption, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40808b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_for_language_specific_overview_num_labels = subset_for_language_specific_overview.pivot(index=[\"Language\"], columns=\"Task\", values=\"Num. of labels\").fillna(\"\").reset_index()\n",
    "rest_columns = [c for c in subset_for_language_specific_overview_num_labels.columns if c not in order_of_tasks_as_abbreviations]\n",
    "subset_for_language_specific_overview_num_labels= subset_for_language_specific_overview_num_labels[rest_columns + [a for a in order_of_tasks_as_abbreviations if a in subset_for_language_specific_overview_num_labels.columns.tolist()]]\n",
    "caption = \"Overview of the number of labels for each language-specific subset of multilingual tasks. The order of the values is train / validation / test.\"\n",
    "label = \"subset_for_language_specific_overview_num_labels\"\n",
    "print(create_latex_sideways_table(subset_for_language_specific_overview_num_labels, index=False, caption=caption, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd7782",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Results from the Literature(10).xlsx\", sheet_name=\"Mono-lingual\")\n",
    "df = df[df[\"Language\"]!='multilingual']\n",
    "\n",
    "df = df[['Model', 'Parameters', 'Vocabulary Size', 'Specs (simple)', 'Pre-Training Corpora (simple)', 'Source']]\n",
    "\n",
    "df = df.rename(columns={'Parameters': 'Params', 'Vocabulary Size': 'Vocab', 'Specs (simple)':'Specs', 'Pre-Training Corpora (simple)':'Corpora'})\n",
    "\n",
    "\n",
    "\n",
    "def insert_citation(exists, x):\n",
    "    if exists != 'Not found':\n",
    "        if x == 'nlpaueb/legal-bert-base-uncased':\n",
    "            return '\\\\cite{'+str('chalkidis_legal-bert_2020')+'}'\n",
    "        else:\n",
    "            return '\\\\cite{'+str(x)+'}'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "df['Source_1'] = df.apply(lambda x: insert_citation(x['Source'], x['Model']), axis=1)\n",
    "\n",
    "del df['Source']\n",
    "\n",
    "df = df.rename(columns={'Source_1': 'Source'})\n",
    "\n",
    "df = df[df.Model.isnull()==False]\n",
    "\n",
    "df['Language'] = df.Model.apply(lambda x: ra.meta_infos['model_language_lookup_table'][x])\n",
    "\n",
    "df['Type']=df.Model.apply(lambda x: check_model_type(x, only_type=True))\n",
    "\n",
    "df = df[[\"Model\",\"Type\",\"Language\",\"Source\", \"Params\", \"Vocab\", \"Specs\"]]\n",
    "\n",
    "\n",
    "\n",
    "df.sort_values(['Type','Language'], inplace=True)\n",
    "\n",
    "\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "df['Specs'] = df.Specs.apply(lambda x: re.sub('k','K',x))\n",
    "\n",
    "df['Specs'] = df.Specs.apply(lambda x: ' / '.join(x.split('/')))\n",
    "\n",
    "caption = 'Monolingual models. BS is short for batch size. For a detailed overview of the pretraining corpora we refer to the publications. For some models we were not able to find publications.'\n",
    "print(create_latex_horizontal_table(df, caption=caption, label=\"overview_monolingual_models\", index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f794390",
   "metadata": {},
   "source": [
    "## 6. Display Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21f015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figures_dir = Path('/Users/vetonmatoshi/Documents/Git/LEXTREME/visualizations/histograms')\n",
    "\n",
    "figures_for_display = list()\n",
    "for path in figures_dir.glob('*/*'):\n",
    "    if str(path).endswith('jpg') and '__' not in str(path):\n",
    "        path_simple = str(path).split('/')[-1]\n",
    "        figures_for_display.append(path_simple)\n",
    "\n",
    "figures_for_display_ordered = list() \n",
    "\n",
    "for x in order_of_tasks_as_abbreviations:\n",
    "    for k,v in ra.meta_infos['task_abbreviations'].items():\n",
    "        if 'multi_eurlex_level_2' not in k and 'multi_eurlex_level_3' not in k:\n",
    "            if x==v:\n",
    "                figures_for_display_ordered.append(k+'.jpg')\n",
    "\n",
    "\n",
    "for f in figures_for_display_ordered:\n",
    "    \n",
    "    caption = 'Histogram for dataset '+ra.meta_infos['task_abbreviations'][f[:-4]]\n",
    "    label = 'histogram_'+ra.meta_infos['task_abbreviations'][f[:-4]]\n",
    "    \n",
    "    latex_impl = list()\n",
    "    latex_impl.append('\\\\begin{figure}[h]')\n",
    "    latex_impl.append('\\\\centering')\n",
    "    latex_impl.append('\\\\includegraphics[width=\\\\textwidth]{plots/'+f+'}')\n",
    "    latex_impl.append('\\\\caption{'+caption+'}')\n",
    "    latex_impl.append('\\\\label{'+label+'}')\n",
    "    latex_impl.append('\\\\end{figure}')\n",
    "    \n",
    "    print('\\n'.join(latex_impl))\n",
    "    print('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13cc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9f8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cef83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5075d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bb0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
